{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f92831-8ae1-44ba-8e04-458295815e39",
   "metadata": {},
   "source": [
    "This notebooks demonstrate how to split data to train-test execute parallel DNN trainings.\n",
    "\n",
    "The example dataset `./example1_data.zarr/` can be generated using this [Jupyter Notebook](https://vegewaterdynamics.github.io/motrainer/notebooks/example_daskml/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985c59a-8284-4f6b-820f-03d818fa1593",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be4e6e8-7757-4874-ba6a-680f497ebafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import motrainer\n",
    "import dask_ml.model_selection as dcv\n",
    "from motrainer.jackknife import JackknifeGPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1dc966-e5c2-48c0-b7c3-48e70642f905",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Read data and split to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5fb223-c59f-408c-ac20-733c296586ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "zarr_file_path = \"./example1_data.zarr\"\n",
    "ds = xr.open_zarr(zarr_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa7df27-8efa-410b-9210-fa83d8425d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(ds):\n",
    "    return ds.to_dask_dataframe()\n",
    "\n",
    "def chunk(ds, chunks):\n",
    "    return ds.chunk(chunks)\n",
    "    \n",
    "bags = motrainer.dataset_split(ds, \"space\")\n",
    "bags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)\n",
    "\n",
    "test_size = 0.33\n",
    "f_shuffle = True\n",
    "train_test_bags = bags.map(\n",
    "    dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1\n",
    ")  \n",
    "train_bags = train_test_bags.pluck(0)\n",
    "test_bags = train_test_bags.pluck(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4eba5d-6dc0-4105-9249-89d85e98b618",
   "metadata": {},
   "source": [
    "## Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e5059d-fe8a-481e-98f3-6d1aacc8c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JackKnife parameters\n",
    "JackKnife = {\n",
    "    'val_split_year': 2017,\n",
    "    'output_list': ['sig', 'slop', 'curv'],\n",
    "    'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],\n",
    "    'out_path': './dnn_examples/results'\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "searching_space = {\n",
    "    'num_dense_layers': [1, 10],\n",
    "    'num_input_nodes': [1, 6],\n",
    "    'num_dense_nodes': [1, 128],\n",
    "    'learning_rate': [5e-4, 1e-2],\n",
    "    'activation': ['relu']\n",
    "}\n",
    "\n",
    "# Here, I reduce parameters to be able to run on my own machine\n",
    "optimize_space = {\n",
    "    'best_loss': 2, # 1\n",
    "    'n_calls': 11, # 15\n",
    "    'epochs': 5, # 300\n",
    "    'noise': 0.1, \n",
    "    'kappa': 5,\n",
    "    'validation_split': 0.2,\n",
    "    'x0': [1e-3, 1, 4, 13, 'relu', 64]\n",
    "} # For weightling loss: 'loss_weights': [1, 1, 0.5], "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1bcc2-8a9b-419a-88fb-862d9b4c8b5b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Run the training\n",
    "\n",
    "In this example, we will demonstrate how to run the training parralel per grid (partition) with a dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41975f9b-15d2-4be0-948f-c29d892073b6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function for training\n",
    "def training_func(gpi_num, df, JackKnife, searching_space, optimize_space):\n",
    "    \n",
    "    # remove NA data\n",
    "    gpi_data = df.compute()\n",
    "    gpi_data.dropna(inplace=True)\n",
    "\n",
    "    # add time to index\n",
    "    gpi_data.set_index(\"time\", inplace=True, drop=True)\n",
    "\n",
    "    gpi = JackknifeGPI(gpi_data,\n",
    "                       JackKnife['val_split_year'],\n",
    "                       JackKnife['input_list'],\n",
    "                       JackKnife['output_list'],\n",
    "                       outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")\n",
    "\n",
    "    gpi.train(searching_space=searching_space,\n",
    "              optimize_space=optimize_space,\n",
    "              normalize_method='standard',\n",
    "              training_method='dnn',\n",
    "              performance_method='rmse',\n",
    "              verbose=2)\n",
    "\n",
    "    gpi.export_best()\n",
    "\n",
    "    return gpi.apr_perf, gpi.post_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e4d43",
   "metadata": {},
   "source": [
    "By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the [Dask Documentation](https://docs.dask.org/en/stable/deploying.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205a69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d24b179-6484-40f3-a1b1-d1f43021c574",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76fef52a-8993-44a4-bf2b-79700070316a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use client to parallelize the loop across workers\n",
    "futures = [\n",
    "    client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags)\n",
    "]\n",
    "\n",
    "# Wait for all computations to finish\n",
    "wait(futures)\n",
    "\n",
    "# Get the results\n",
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12e262ee-3d20-4d4b-b889-48d06549735b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close the Dask client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c93bf79b-723c-465a-8f06-e859909ac56a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPI 1\n",
      " aprior performance(RMSE):\n",
      "[[0.26438]\n",
      " [0.03295]\n",
      " [0.14362]]\n",
      "post performance(RMSE):\n",
      "[[0.34483]\n",
      " [0.00631]\n",
      " [0.00686]]\n",
      "=========================================\n",
      "GPI 2\n",
      " aprior performance(RMSE):\n",
      "[[0.37801]\n",
      " [0.02598]\n",
      " [0.26245]]\n",
      "post performance(RMSE):\n",
      "[[0.70249]\n",
      " [0.22075]\n",
      " [0.24423]]\n",
      "=========================================\n",
      "GPI 3\n",
      " aprior performance(RMSE):\n",
      "[[0.31875]\n",
      " [0.24323]\n",
      " [0.05353]]\n",
      "post performance(RMSE):\n",
      "[[0.03498]\n",
      " [0.19958]\n",
      " [0.24324]]\n",
      "=========================================\n",
      "GPI 4\n",
      " aprior performance(RMSE):\n",
      "[[0.19431]\n",
      " [0.10026]\n",
      " [0.16398]]\n",
      "post performance(RMSE):\n",
      "[[0.20526]\n",
      " [0.02813]\n",
      " [0.21003]]\n",
      "=========================================\n",
      "GPI 5\n",
      " aprior performance(RMSE):\n",
      "[[0.23724]\n",
      " [0.1104 ]\n",
      " [0.28052]]\n",
      "post performance(RMSE):\n",
      "[[0.10751]\n",
      " [0.08874]\n",
      " [0.26091]]\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "for gpi_num, performance in enumerate(results):\n",
    "    print(f\"GPI {(gpi_num + 1)}\")\n",
    "    print(\" aprior performance(RMSE):\")\n",
    "    print(performance[0])\n",
    "    print(\"post performance(RMSE):\")\n",
    "    print(performance[1])\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf787f-af84-4caa-b999-1a82ba8984b7",
   "metadata": {},
   "source": [
    "Shutdown the client to free up the resources click on SHUTDOWN in the Dask JupyterLab extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e498d-e3e9-435a-b88e-f8cc869cd680",
   "metadata": {},
   "source": [
    "## Inspect best model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e312cb1f-9eb6-436e-b052-0570cbab910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d68530-5a79-4f17-9d92-539999939940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 45        \n",
      "                                                                 \n",
      " layer_dense_1 (Dense)       (None, 39)                234       \n",
      "                                                                 \n",
      " layer_dense_2 (Dense)       (None, 39)                1560      \n",
      "                                                                 \n",
      " layer_dense_3 (Dense)       (None, 39)                1560      \n",
      "                                                                 \n",
      " layer_dense_4 (Dense)       (None, 39)                1560      \n",
      "                                                                 \n",
      " layer_dense_5 (Dense)       (None, 39)                1560      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 120       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,639\n",
      "Trainable params: 6,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = \"./dnn_examples/results/gpi1/best_optimized_model_2015.h5\"\n",
    "model = tf.keras.models.load_model(best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6def91-746f-4c27-a887-1303b4bd4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more info to the model file e.g. the path to the data\n",
    "with h5py.File(best_model, 'a') as f:\n",
    "    f.attrs['input_file_path'] = \"./example1_data.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6f431f-33d8-4393-9182-cf55997938af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.6945188437821344e-05, [0.0102933544822095, 2, 6, 6, 'relu', 220]), (0.0009668049169704318, [0.014053549021147586, 1, 5, 5, 'relu', 213]), (0.0011984826996922493, [0.01, 2, 5, 5, 'relu', 64]), (0.002951781963929534, [0.01034267518524073, 1, 6, 5, 'relu', 325]), (0.005780111066997051, [0.01, 1, 5, 6, 'relu', 189]), (0.0062195612117648125, [0.01238696509850767, 1, 5, 6, 'relu', 89]), (0.007209620904177427, [0.012213029952058047, 1, 6, 5, 'relu', 41]), (0.008430225774645805, [0.016252143020123296, 1, 5, 6, 'relu', 153]), (0.01519404910504818, [0.012097401107848887, 1, 5, 6, 'relu', 161]), (0.03946790099143982, [0.013412574345977837, 2, 5, 5, 'relu', 242]), (0.04046712443232536, [0.010949417026328035, 2, 5, 5, 'relu', 276]), (0.09059736132621765, [0.013515310361619494, 1, 6, 6, 'relu', 147]), (0.11357539147138596, [0.013742318889904532, 2, 5, 6, 'relu', 46]), (0.11388207972049713, [0.02, 1, 5, 6, 'relu', 123]), (0.7128570675849915, [0.012739986706827862, 1, 6, 6, 'relu', 302])]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the hyperparameters and input_list \n",
    "with h5py.File(best_model, 'r') as f:\n",
    "    hyperparameters = f.attrs['hyperparameters']\n",
    "    input_list = f.attrs['input_list']\n",
    "    input_file_path = f.attrs['input_file_path']\n",
    "\n",
    "print(eval(hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df2125e9-7589-4239-87aa-73d4a0751153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG1' 'TG2' 'TG3' 'WG1' 'WG2' 'WG3' 'BIOMA1' 'BIOMA2']\n"
     ]
    }
   ],
   "source": [
    "print(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed02eff-fcb3-4cb3-a381-49707aa3e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example1_data.zarr\n"
     ]
    }
   ],
   "source": [
    "print(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d21ae-c37a-4c2f-becd-fe8b0c22a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
