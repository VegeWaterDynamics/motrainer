{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f92831-8ae1-44ba-8e04-458295815e39",
   "metadata": {},
   "source": [
    "This notebooks demonstrate how to split data to train-test execute parallel DNN trainings.\n",
    "\n",
    "The example dataset `./example1_data.zarr/` can be generated using the following Jupyter Notebook:\n",
    "- [Covert a nested DataFrame to a Dataset](../example_read_from_one_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985c59a-8284-4f6b-820f-03d818fa1593",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be4e6e8-7757-4874-ba6a-680f497ebafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 15:01:18.501959: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-10 15:01:18.511230: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-10 15:01:18.616501: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 15:01:18.616572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 15:01:18.624625: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-10 15:01:18.644226: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-10 15:01:18.646772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-10 15:01:20.706954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import motrainer\n",
    "import dask_ml.model_selection as dcv\n",
    "from motrainer.jackknife import JackknifeGPI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1dc966-e5c2-48c0-b7c3-48e70642f905",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Read data and split to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5fb223-c59f-408c-ac20-733c296586ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "zarr_file_path = \"./example1_data.zarr\"\n",
    "ds = xr.open_zarr(zarr_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f19916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use less data to reduce training time\n",
    "ds = ds.isel(time=ds.time>=np.datetime64('2015-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa7df27-8efa-410b-9210-fa83d8425d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(ds):\n",
    "    return ds.to_dask_dataframe()\n",
    "\n",
    "def chunk(ds, chunks):\n",
    "    return ds.chunk(chunks)\n",
    "    \n",
    "bags = motrainer.dataset_split(ds, \"space\")\n",
    "bags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)\n",
    "\n",
    "test_size = 0.33\n",
    "f_shuffle = True\n",
    "train_test_bags = bags.map(\n",
    "    dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1\n",
    ")  \n",
    "train_bags = train_test_bags.pluck(0)\n",
    "test_bags = train_test_bags.pluck(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4eba5d-6dc0-4105-9249-89d85e98b618",
   "metadata": {},
   "source": [
    "## Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e5059d-fe8a-481e-98f3-6d1aacc8c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JackKnife parameters\n",
    "JackKnife = {\n",
    "    'val_split_year': 2017,\n",
    "    'output_list': ['sig', 'slop', 'curv'],\n",
    "    'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],\n",
    "    'out_path': './dnn_examples/results'\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "searching_space = {\n",
    "    'num_dense_layers': [1, 2],\n",
    "    'num_input_nodes': [2, 3],\n",
    "    'num_dense_nodes': [16, 32],\n",
    "    'learning_rate': [1e-3, 1e-2],\n",
    "    'activation': ['relu']\n",
    "}\n",
    "\n",
    "# Define the optimization\n",
    "optimize_space = {\n",
    "    'best_loss': 2,\n",
    "    'n_calls': 11,\n",
    "    'epochs': 2,\n",
    "    'noise': 0.1, \n",
    "    'kappa': 5,\n",
    "    'validation_split': 0.2,\n",
    "    'x0': [1e-3, 1, 2, 16, 'relu', 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1bcc2-8a9b-419a-88fb-862d9b4c8b5b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Run the training\n",
    "\n",
    "In this example, we will demonstrate how to run the training parralel per grid (partition) with a dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41975f9b-15d2-4be0-948f-c29d892073b6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function for training\n",
    "def training_func(gpi_num, df, JackKnife, searching_space, optimize_space):\n",
    "    \n",
    "    # remove NA data\n",
    "    gpi_data = df.compute()\n",
    "    gpi_data.dropna(inplace=True)\n",
    "\n",
    "    # add time to index\n",
    "    gpi_data.set_index(\"time\", inplace=True, drop=True)\n",
    "\n",
    "    gpi = JackknifeGPI(gpi_data,\n",
    "                       JackKnife['val_split_year'],\n",
    "                       JackKnife['input_list'],\n",
    "                       JackKnife['output_list'],\n",
    "                       outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")\n",
    "\n",
    "    gpi.train(searching_space=searching_space,\n",
    "              optimize_space=optimize_space,\n",
    "              normalize_method='standard',\n",
    "              training_method='dnn',\n",
    "              performance_method='rmse',\n",
    "              verbose=2)\n",
    "\n",
    "    gpi.export_best()\n",
    "\n",
    "    return gpi.apr_perf, gpi.post_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e4d43",
   "metadata": {},
   "source": [
    "By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the [Dask Documentation](https://docs.dask.org/en/stable/deploying.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205a69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d24b179-6484-40f3-a1b1-d1f43021c574",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76fef52a-8993-44a4-bf2b-79700070316a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use client to parallelize the loop across workers\n",
    "futures = [\n",
    "    client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags)\n",
    "]\n",
    "\n",
    "# Wait for all computations to finish\n",
    "wait(futures)\n",
    "\n",
    "# Get the results\n",
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12e262ee-3d20-4d4b-b889-48d06549735b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Close the Dask client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c93bf79b-723c-465a-8f06-e859909ac56a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPI 1\n",
      " aprior performance(RMSE):\n",
      "[[0.03349]\n",
      " [0.07387]\n",
      " [0.22965]]\n",
      "post performance(RMSE):\n",
      "[[0.32417]\n",
      " [0.08714]\n",
      " [0.81276]]\n",
      "=========================================\n",
      "GPI 2\n",
      " aprior performance(RMSE):\n",
      "[[0.10507]\n",
      " [0.03492]\n",
      " [0.09597]]\n",
      "post performance(RMSE):\n",
      "[[0.0185 ]\n",
      " [0.77383]\n",
      " [0.20172]]\n",
      "=========================================\n",
      "GPI 3\n",
      " aprior performance(RMSE):\n",
      "[[0.32753]\n",
      " [0.36519]\n",
      " [0.26186]]\n",
      "post performance(RMSE):\n",
      "[[0.17438]\n",
      " [0.26897]\n",
      " [0.16316]]\n",
      "=========================================\n",
      "GPI 4\n",
      " aprior performance(RMSE):\n",
      "[[0.22702]\n",
      " [0.50275]\n",
      " [0.12853]]\n",
      "post performance(RMSE):\n",
      "[[0.48915]\n",
      " [0.08741]\n",
      " [0.4903 ]]\n",
      "=========================================\n",
      "GPI 5\n",
      " aprior performance(RMSE):\n",
      "[[0.34393]\n",
      " [0.1475 ]\n",
      " [0.25872]]\n",
      "post performance(RMSE):\n",
      "[[2.02418]\n",
      " [0.47361]\n",
      " [0.67175]]\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "from pathlib import Path\n",
    "Path('./results').mkdir(exist_ok=True)\n",
    "for gpi_num, performance in enumerate(results):\n",
    "    print(f\"GPI {(gpi_num + 1)}\")\n",
    "    print(\" aprior performance(RMSE):\")\n",
    "    print(performance[0])\n",
    "    print(\"post performance(RMSE):\")\n",
    "    print(performance[1])\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf787f-af84-4caa-b999-1a82ba8984b7",
   "metadata": {},
   "source": [
    "Shutdown the client to free up the resources click on SHUTDOWN in the Dask JupyterLab extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e498d-e3e9-435a-b88e-f8cc869cd680",
   "metadata": {},
   "source": [
    "## Inspect best model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e312cb1f-9eb6-436e-b052-0570cbab910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d68530-5a79-4f17-9d92-539999939940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2)                 18        \n",
      "                                                                 \n",
      " layer_dense_1 (Dense)       (None, 19)                57        \n",
      "                                                                 \n",
      " layer_dense_2 (Dense)       (None, 19)                380       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 60        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 515 (2.01 KB)\n",
      "Trainable params: 515 (2.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = \"./dnn_examples/results/gpi1/best_optimized_model_2015.h5\"\n",
    "model = tf.keras.models.load_model(best_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b6def91-746f-4c27-a887-1303b4bd4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more info to the model file e.g. the path to the data\n",
    "with h5py.File(best_model, 'a') as f:\n",
    "    f.attrs['input_file_path'] = \"./example1_data.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a6f431f-33d8-4393-9182-cf55997938af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5740020871162415, [0.009248990393121144, 2, 2, 19, 'relu', 219]), (0.6381517052650452, [0.0034642993935407942, 2, 3, 19, 'relu', 89]), (0.7027523517608643, [0.00943332323418086, 2, 2, 25, 'relu', 306]), (0.7084829211235046, [0.006084925609967853, 1, 2, 27, 'relu', 123]), (0.7407179474830627, [0.002975229104493618, 1, 2, 31, 'relu', 122]), (0.7598923444747925, [0.0018202265892128732, 1, 2, 22, 'relu', 59]), (0.7676792144775391, [0.0012450228647412386, 1, 2, 18, 'relu', 330]), (0.7816160917282104, [0.0023138826233784302, 2, 2, 25, 'relu', 38]), (0.8949440121650696, [0.001439410716322745, 1, 3, 17, 'relu', 103]), (0.9745810627937317, [0.001, 1, 2, 16, 'relu', 32]), (1.0652109384536743, [0.001534802942029055, 2, 3, 16, 'relu', 295])]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the hyperparameters and input_list \n",
    "with h5py.File(best_model, 'r') as f:\n",
    "    hyperparameters = f.attrs['hyperparameters']\n",
    "    input_list = f.attrs['input_list']\n",
    "    input_file_path = f.attrs['input_file_path']\n",
    "\n",
    "print(eval(hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df2125e9-7589-4239-87aa-73d4a0751153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TG1' 'TG2' 'TG3' 'WG1' 'WG2' 'WG3' 'BIOMA1' 'BIOMA2']\n"
     ]
    }
   ],
   "source": [
    "print(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ed02eff-fcb3-4cb3-a381-49707aa3e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example1_data.zarr\n"
     ]
    }
   ],
   "source": [
    "print(input_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
