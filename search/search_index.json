{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MOTrainer","text":"<p>Measurement Operator Trainer (MOTrainer) is a Python package which facilitates parallel training processes of measurement operators (MO) for Data Assimilation (DA) purposes. It specifically address the need of parallely training multiple MOs over independent partitions of large spatio-temporal dataset.</p> <p>MOTrainer provices functionality to split spatio-temporal datasets for independent training processes. It utilizes Xarray's feature of multi-dimensional labelling to address the spatio-temporal characteres of the input/output datasets. Then Dask is implemented to achive the parallel training jobs.  </p> <p>MOTrainer supports training simple structured Machine Learning (ML) models which can be trained using SciKit-Learn toolkits. It also provide supports on parallel training Training DeepNeuron Networks with TensorFlow.</p>"},{"location":"CHANGELOG/","title":"Change Log","text":"<p>All notable changes to this project will be documented in this file. This project adheres to Semantic Versioning.</p> <p>[0.1.0] - 2023-12-21</p>"},{"location":"CHANGELOG/#added","title":"Added","text":"<p>The first version of the MOTrainer package. The following functionalities are implemented:</p> <ul> <li>Spatio temporal Dataset split;</li> <li>Jackknife GPI implementation;</li> <li>Documentation and examples for distributed training;</li> <li>Relevant tests.</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>This code of conduct is adapted from the  Git Code of Conduct.</p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at team-atlas@esciencecenter.nl.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All Project maintainers are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4,  available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"MOTrainer Contributing Guidelines","text":"<p>We welcome any kind of contribution to our software, from simple comment  or question to a full fledged pull request.  Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ul> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation).</li> </ul> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you-have-a-question","title":"You have a question","text":"<ul> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"question\" label; apply other labels when relevant.</li> </ul>"},{"location":"CONTRIBUTING/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ul> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>apply relevant labels to the newly created issue.</li> </ul>"},{"location":"CONTRIBUTING/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ul> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>make sure the existing tests still work. First, install the development dependencies as <code>pip install .[dev]</code>, and then run <code>pytest tests</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation. Make sure the documentation is built successfully: first, install documentation dependencies as <code>pip install .[docs]</code> and then run <code>mkdocs build</code>.</li> <li>make sure the linting tests pass by running <code>ruff</code> in the project root directory: <code>ruff check .</code>;</li> <li>push your feature branch to (your fork of) the MOTrainer repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ul> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"setup/","title":"Installation","text":"<p>MOTrainer can be installed from PyPI:</p> <pre><code>pip install motrainer\n</code></pre> <p>or from the source:</p> <pre><code>git clone git@github.com:VegeWaterDynamics/motrainer.git\ncd motrainer\npip install .\n</code></pre> <p>Note that Python version <code>&gt;=3.10</code> and <code>&lt;3.12</code> is required to install MOTrainer.</p>"},{"location":"setup/#tips","title":"Tips","text":"<p>We strongly recommend installing separately from your default Python envrionment. E.g. you can use enviroment manager e.g. mamba to create separate environment.</p>"},{"location":"usage_daskml/","title":"Parallel ML Model Optimization with Dask-ml","text":"<p>In the data split section, we discussed how to partition a dataset. After the partitioning process, the dataset is divided into two data bags: <code>train_bags</code> and <code>test_bags</code>.</p> <p>Following dataset partitioning, you can execute parallel machine learning training using <code>dask-ml</code>. Dask-ml leverages Dask alongside popular machine learning libraries like Scikit-Learn to facilitate distributed machine learning tasks. This document provides a brief overview of its usage. For a more comprehensive example, refer to this Jupyter Notebook.</p>"},{"location":"usage_daskml/#extract-training-data","title":"Extract training data","text":"<p>With <code>dask-ml</code>, the input and output data need be converted to a DataFrame. You can map this conversion to each partition as follows:</p> <pre><code>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n\ntrain_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe)\n</code></pre>"},{"location":"usage_daskml/#configuring-the-search-grid","title":"Configuring the Search Grid","text":"<p>You can set up distributed machine learning training jobs using <code>dask-ml.model_selection</code>. For instance, you can perform an exhaustive search over specified parameter values for an estimator using <code>GridSearchCV</code>:</p> <pre><code>import dask_ml.model_selection as dcv\nfrom sklearn import svm, datasets\n\nparameters = {'kernel': ['linear', 'rbf'], 'C': [1, 10]}\nsvc = svm.SVC()\nsearch = dcv.GridSearchCV(svc, parameters)\n</code></pre>"},{"location":"usage_daskml/#executing-grid-search-in-parallel","title":"Executing Grid Search in Parallel","text":"<p>After setting up the search object, you can map the <code>fit</code> function to each partition as follows:</p> <pre><code>def optimize(df, grid_search, input_list, output_list):\n    \"\"\"Customized Optimization Function\n    \"\"\"\n    grid_result = grid_search.fit(df[input_list], df[output_list])\n    return grid_result\n\ninput_list = [\"STATE1\", \"STATE2\", \"STATE3\", \"STATE4\", \"STATE5\"]\noutput_list = [\"slop\"]\n\n# When memory allows, peresist data in the workers\n# When train_bags can is lazilly loading from file system, this can avoid redundant data loading\ntrain_bags = train_bags.persist()\n\noptimazed_estimators = train_bags.map(\n    optimize, search, input_list, output_list\n)\n</code></pre> <p>In this way, the machine learning training task is parallelized on two levels:</p> <ol> <li>The training jobs of all partitions of the dataset are parallelized;</li> <li>The parameter searching within each partition is parallelized.</li> </ol> <p>By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the Dask Documentation.</p>"},{"location":"usage_dnn/","title":"Parallel DNN Training with JackknifeGPI","text":"<p>The data split section explains how to partition a dataset. After partitioning, the dataset is divided into two data bags: <code>train_bags</code> and <code>test_bags</code>.</p> <p>This section demonstrates how to perform distributed DNN training using Tensorflow, with Jackknife resampling as the cross-validation method. In this approach, each year is iteratively left out as the cross-validation data. The best model is selected based on the lowest Root Mean Square Error (RMSE).</p> <p>A more comprehensive example can be found in this Example Notebook.</p>"},{"location":"usage_dnn/#training-a-single-grid-point","title":"Training a Single Grid Point","text":"<p>To train a DNN for a single grid cell, you can use the <code>JackknifeGPI</code> object:</p> <pre><code>from motrainer.jackknife import JackknifeGPI\n\n# Intiate a Jackknife GPI from one gridcell\ndf = train_bags.take(1)\ngpi_data = df.compute()\ngpi = JackknifeGPI(gpi_data, outpath='./results')\n\n# Perform training and export\nresults = gpi.train()\ngpi.export_best()\n</code></pre> <p>The training results will be exported to the <code>./results</code>path.</p>"},{"location":"usage_dnn/#training-multiple-grid-points","title":"Training Multiple Grid Points","text":"<p>To train multiple grid points in parallel, you can define a training function as follows:</p> <pre><code>def training_func(gpi_num, df):\n    gpi_data = df.compute()\n\n    gpi = JackknifeGPI(gpi_data,\n                       outpath=f\"results/gpi{gpi_num}\")\n\n    gpi.train()\n    gpi.export_best()\n</code></pre> <p>Then, map the training function to each grid cell:</p> <pre><code>from dask.distributed import Client, wait\n\n# Use client to parallelize the loop across workers\nclient = Client()\nfutures = [\n    client.submit(training_func, gpi_num, df) for gpi_num, df in enumerate(train_bags)\n]\n\n# Wait for all computations to finish\nwait(futures)\n\n# Get the results\nresults = client.gather(futures)\n</code></pre> <p>The above examples uses a local threaded Dask scheduler to parallelize the tasks. When executing training on an HPC system, we recommend using Dask SLURM cluster for the distributed training. For more information on different Dask clusters, please check the Dask Documentation.</p> <p>You can also directly submit training jobs as SLURM jobs, instead of using Dask SLURM cluster. You can find the example of using SLURM here.</p>"},{"location":"usage_split/","title":"Data Split","text":"<p>This section demonstrates how to use motrainer to partition data for parallel ML model training.</p> <p>We start with an xarray.Dataset object, which contains spatio-temporal data for training multiple independent Machine Learning (ML) models. Examples of reading spatio-temporal data into <code>xarray.Dataset</code> can be found in Example 1 and Example 2.</p> <p>The example Dataset object <code>ds</code> contains input and output data from 5 grid cells. It has two dimensions: <code>space</code> and <code>time</code>. It has six data variables, where <code>STATE1</code> to <code>STATE5</code> are physical model states (input data) and observations (output data).</p> <pre><code>print(ds)\n</code></pre> <pre><code>&lt;xarray.Dataset&gt;\nDimensions:    (space: 5, time: 8506)\nCoordinates:\n    latitude   (space) float64 56.12 46.12 53.38 49.38 44.38\n    longitude  (space) float64 11.38 6.625 6.125 12.38 0.625\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\nDimensions without coordinates: space\nData variables:\n    STATE1     (space, time) float64 0.07079 0.05532 0.04846 ... 0.06712 0.05521\n    STATE2     (space, time) float64 0.04366 0.0462 0.03821 ... 0.0861 0.05622\n    STATE3     (space, time) float64 280.0 270.4 285.5 277.4 ... 287.4 272.1\n    STATE4     (space, time) float64 274.8 278.4 280.6 283.7 ... 280.2 281.5\n    STATE5     (space, time) float64 280.9 279.7 278.0 278.0 ... 281.2 280.1\n    OBS        (space, time) float64 -9.49 -8.494 -9.069 ... -8.071 -8.237\nAttributes:\n    license:  data license\n    source:   data source\n</code></pre> <p>Before splitting, you can verify if a dataset is splittable using the <code>is_splitable</code> function:</p> <pre><code>motrainer.is_splitable(ds)\n</code></pre> <pre><code>True\n</code></pre> <p>The is_splitable function will return True if the dataset has exactly two dimensions: \"space\" and \"time\", and there are no duplicated keys in any of the coordinates of the dataset.</p>"},{"location":"usage_split/#splitting-spatio-temporal-dataset-for-independent-training-processes","title":"Splitting Spatio-Temporal Dataset for Independent Training Processes","text":"<p>The dataset_split function can be used to partition data. The split can be performed by specifying a dimension name or using an identifier.</p>"},{"location":"usage_split/#splitting-by-dimension-names","title":"Splitting by Dimension Names","text":"<p>One can specify adimension name to split, e.g. \"space\":</p> <p><pre><code>import motrainer\nbags = motrainer.dataset_split(ds, \"space\")\nprint(bags)\n</code></pre> <pre><code>dask.bag&lt;from_sequence, npartitions=5&gt;\n</code></pre></p> <p>This will split <code>ds</code> per grid cell in to a <code>Dask.bag</code> object. Each partition is an independent gridcell. </p> <p>We can check one grid cell by:</p> <p><pre><code>print(bags.take(1))\n</code></pre> <pre><code>(&lt;xarray.Dataset&gt;\n Dimensions:    (space: 1, time: 8506)\n Coordinates:\n     latitude   (space) float64 56.12\n     longitude  (space) float64 11.38\n   * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\n     space_id   (space) int64 0\n Dimensions without coordinates: space\n Data variables:\n     STATE1     (space, time) float64 0.07079 0.05532 0.04846 ... 0.06611 0.06511\n     STATE2     (space, time) float64 0.04366 0.0462 0.03821 ... 0.0361 0.05612\n     STATE3     (space, time) float64 280.0 270.4 285.5 277.4 ... 282.4 278.7\n     STATE4     (space, time) float64 274.8 278.4 280.6 283.7 ... 281.2 281.9\n     STATE5     (space, time) float64 280.9 279.7 278.0 278.0 ... 276.1 279.1\n     OBS        (space, time) float64 -9.49 -8.494 -9.069 ... -8.1721 -8.157\n Attributes:\n     license:  data license\n     source:   data source,)\n</code></pre></p>"},{"location":"usage_split/#1-d-split-by-indetifier","title":"1-D split by indetifier","text":"<p>One can also create an identifier dictionary to split. The keys of the dictionary should be a subset of {\"space\", \"time\"}, mapping \"space\" and/or \"time\" dimension with corresponding separation identifier.</p> <p>For example, <code>ds</code> can be splitted into two parts (first+fourth grid, and the rest) in space:</p> <pre><code>import numpy as np\n\nidentifier = {\"space\": np.array([0, 1, 1, 0, 1])}\nbags = motrainer.dataset_split(ds, identifier)\n\nds_splitted = bags.compute()\nprint(ds_splitted[0].dims['space'])\nprint(ds_splitted[1].dims['space'])\n</code></pre> <pre><code>2\n3\n</code></pre>"},{"location":"usage_split/#2-d-split-by-indetifier","title":"2-D split by indetifier","text":"<p>One can perform a 2-D split by providing identifiers in both space and time dimensions:</p> <p><pre><code>id_time = np.zeros(8506)\nid_time[2000:]=1\nidentifier = {\"space\": np.array([0, 1, 1, 0, 1]), \"time\": id_time}\nbags = motrainer.dataset_split(ds, identifier)\n\nprint(bags)\n</code></pre> <pre><code>dask.bag&lt;from_sequence, npartitions=4&gt;\n</code></pre></p> <p><pre><code>for parts in ds_splitted:\n    print(parts.dims)\n</code></pre> <pre><code>Frozen({'samples': 4000})\nFrozen({'samples': 13012})\nFrozen({'samples': 6000})\nFrozen({'samples': 19518})\n</code></pre></p>"},{"location":"usage_split/#train-test-split","title":"Train-Test Split","text":"<p>Before training, one can further split the datasets into training and testing datasets to reserve some data for testing. If the split needs to be based on the space and time coordinates, one can use <code>motrainer.train_test_split</code> to perform this split. Otherwise we recommend <code>sklearn.model_selection.train_test_split</code>(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for simplicity.</p>"},{"location":"usage_split/#splitting-by-coordinates","title":"Splitting by Coordinates","text":"<p>When specifying a space or time coordinate, the dataset will be split into two parts: the part smaller (\"&lt;\") than the coordinates and the rest (\"&gt;=\"). By default, the former will be the training data and the latter will be testing.</p> <p>If the dataset has already be splitted into <code>Dask.bags</code>, we recommend to use the <code>map</code> function to apply train-test split to each splitted element.</p> <p>The following will select data before 2017-01-01 as training data:</p> <pre><code>train_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2017-01-01\")}\n)\n</code></pre> <p>Then extract train and test data using <code>pluck</code>:</p> <pre><code>train_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</code></pre> <p>When <code>reverse=True</code> is present, the latter part (\"&gt;=\" coordinate) will be training data, the rest will be testing. The following code will select data after (and include) 2017-01-01 as training data:</p> <pre><code>train_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2017-01-01\"), reverse=True}\n)\ntrain_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</code></pre> <p>One can also apply <code>motrainer.train_test_split</code> directly to an <code>xarray.Dataset</code> object: <pre><code>motrainer.train_test_split(ds, split={\"time\": np.datetime64(\"2017-01-01\")})\n</code></pre></p>"},{"location":"usage_split/#splitting-by-mask","title":"Splitting by Mask","text":"<p>Alternatively, you can also initiate a <code>mask</code> to perform training data. By default, training data will be where <code>mask</code> is <code>True</code>. For example, if you would like to have data before 2017-01-01 as training data:</p> <pre><code>mask = ds_valid[\"time\"] &lt; np.datetime64(\"2017-01-01\")\ntrain, test = train_test_split(ds_valid, mask=mask)\n</code></pre> <p>If <code>reverse</code> is specified, training data will be where mask is <code>False</code>. The following will select data after (and include) 2017-01-01 as training data:</p> <pre><code>mask = ds_valid[\"time\"] &lt; np.datetime64(\"2017-01-01\")\ntrain, test = train_test_split(ds_valid, mask=mask, reverse=True)\n</code></pre>"},{"location":"notebooks/example_daskml/","title":"Prallely training sklearn models with dask-ml","text":"<p>This notebooks demonstrate how to execute parallel machine learning training using <code>dask-ml</code> and motrainer.</p> <p>The example dataset <code>./example1_data.zarr/</code> can be generated using this Jupyter Notebook.</p> In\u00a0[1]: Copied! <pre>import motrainer\nimport numpy as np\nimport xarray as xr\n</pre> import motrainer import numpy as np import xarray as xr In\u00a0[2]: Copied! <pre>ds = xr.open_zarr(\"./example1_data.zarr/\")\nds\n</pre> ds = xr.open_zarr(\"./example1_data.zarr/\") ds Out[2]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (space: 5, time: 8506)\nCoordinates:\n    latitude   (space) float64 dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;\n    longitude  (space) float64 dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\nDimensions without coordinates: space\nData variables:\n    BIOMA1     (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    BIOMA2     (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG1        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG2        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG3        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG1        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG2        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG3        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    curv       (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    sig        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    slop       (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\nAttributes:\n    license:  data license\n    source:   data source</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>space: 5</li><li>time: 8506</li></ul></li><li>Coordinates: (3)<ul><li>latitude(space)float64dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;  Array   Chunk   Bytes   40 B   40 B   Shape   (5,)   (5,)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  5 1 </li><li>longitude(space)float64dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;  Array   Chunk   Bytes   40 B   40 B   Shape   (5,)   (5,)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  5 1 </li><li>time(time)datetime64[ns]2007-01-02 ... 2020-01-01T01:00:00<pre>array(['2007-01-02T00:00:00.000000000', '2007-01-03T00:00:00.000000000',\n       '2007-01-03T00:30:00.000000000', ..., '2020-01-01T00:00:00.000000000',\n       '2020-01-01T00:30:00.000000000', '2020-01-01T01:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (11)<ul><li>BIOMA1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>BIOMA2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG3(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG3(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>curv(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>sig(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>slop(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2007-01-02 00:00:00', '2007-01-03 00:00:00',\n               '2007-01-03 00:30:00', '2007-01-05 00:00:00',\n               '2007-01-06 00:00:00', '2007-01-08 00:00:00',\n               '2007-01-08 00:30:00', '2007-01-09 00:00:00',\n               '2007-01-10 00:00:00', '2007-01-11 00:00:00',\n               ...\n               '2019-12-29 01:00:00', '2019-12-30 00:00:00',\n               '2019-12-30 00:30:00', '2019-12-30 01:00:00',\n               '2019-12-30 01:30:00', '2019-12-31 00:00:00',\n               '2019-12-31 00:30:00', '2020-01-01 00:00:00',\n               '2020-01-01 00:30:00', '2020-01-01 01:00:00'],\n              dtype='datetime64[ns]', name='time', length=8506, freq=None))</pre></li></ul></li><li>Attributes: (2)license :data licensesource :data source</li></ul> In\u00a0[3]: Copied! <pre># Check if the dataset is splitable\nmotrainer.is_splitable(ds)\n</pre> # Check if the dataset is splitable motrainer.is_splitable(ds) Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># split the dataset per grid cell\nbags = motrainer.dataset_split(ds, \"space\")\nbags\n</pre> # split the dataset per grid cell bags = motrainer.dataset_split(ds, \"space\") bags Out[4]: <pre>dask.bag&lt;from_sequence, npartitions=5&gt;</pre> In\u00a0[5]: Copied! <pre>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n</pre> def to_dataframe(ds):     return ds.to_dask_dataframe()  def chunk(ds, chunks):     return ds.chunk(chunks) In\u00a0[6]: Copied! <pre># Train test split, mapped to each element of the bag\ntrain_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2016-01-01\")}\n)\n\n# # Or split by mask\n# mask = ds[\"time\"]&lt;np.datetime64(\"2016-01-01\")\n# train_test_bags = bags.map(\n#     motrainer.train_test_split, mask={\"time\": np.datetime64(\"2016-01-01\")}\n# )\n</pre> # Train test split, mapped to each element of the bag train_test_bags = bags.map(     motrainer.train_test_split, split={\"time\": np.datetime64(\"2016-01-01\")} )  # # Or split by mask # mask = ds[\"time\"] In\u00a0[7]: Copied! <pre># Retrieve the train and test bags\ntrain_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe)\ntest_bags = train_test_bags.pluck(1).map(chunk, {\"space\": 500}).map(to_dataframe)\n</pre> # Retrieve the train and test bags train_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe) test_bags = train_test_bags.pluck(1).map(chunk, {\"space\": 500}).map(to_dataframe) In\u00a0[8]: Copied! <pre># Setup grid search\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RepeatedKFold\n\nfrom dask_ml.preprocessing import MinMaxScaler\nfrom dask_ml.model_selection import GridSearchCV\n\nregSVR = make_pipeline(MinMaxScaler(), SVR())\nkernel = [\"poly\", \"rbf\", \"sigmoid\"]\nC = [1, 0.1]\ngamma = [\"scale\"]\ngrid = dict(svr__kernel=kernel, svr__C=C, svr__gamma=gamma)\ncv = RepeatedKFold(n_splits=4, n_repeats=2, random_state=1)\ngrid_search = GridSearchCV(\n    estimator=regSVR,\n    param_grid=grid,\n    cv=cv,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    refit=\"r2\",\n)\n</pre> # Setup grid search from sklearn.svm import SVR from sklearn.pipeline import make_pipeline from sklearn.model_selection import RepeatedKFold  from dask_ml.preprocessing import MinMaxScaler from dask_ml.model_selection import GridSearchCV  regSVR = make_pipeline(MinMaxScaler(), SVR()) kernel = [\"poly\", \"rbf\", \"sigmoid\"] C = [1, 0.1] gamma = [\"scale\"] grid = dict(svr__kernel=kernel, svr__C=C, svr__gamma=gamma) cv = RepeatedKFold(n_splits=4, n_repeats=2, random_state=1) grid_search = GridSearchCV(     estimator=regSVR,     param_grid=grid,     cv=cv,     scoring=[\"r2\", \"neg_mean_squared_error\"],     refit=\"r2\", ) In\u00a0[9]: Copied! <pre># Setup optimization function\ndef optimize(df, grid_search, input_list, output_list):\n    \"\"\"Customized Optimization Function\n    \"\"\"\n    df = df.dropna()\n    grid_result = grid_search.fit(df[input_list], df[output_list])\n    return grid_result\n\n# Map the optimization function to the train bags\ninput_list = [\"BIOMA1\", \"BIOMA1\", \"TG1\", \"TG2\", \"TG3\"]\noutput_list = [\"slop\"]\noptimazed_estimators = train_bags.map(\n    optimize, grid_search=grid_search, input_list=input_list, output_list=output_list\n)\n</pre> # Setup optimization function def optimize(df, grid_search, input_list, output_list):     \"\"\"Customized Optimization Function     \"\"\"     df = df.dropna()     grid_result = grid_search.fit(df[input_list], df[output_list])     return grid_result  # Map the optimization function to the train bags input_list = [\"BIOMA1\", \"BIOMA1\", \"TG1\", \"TG2\", \"TG3\"] output_list = [\"slop\"] optimazed_estimators = train_bags.map(     optimize, grid_search=grid_search, input_list=input_list, output_list=output_list ) In\u00a0[12]: Copied! <pre># Execute the training\noptimazed_estimators_realized = optimazed_estimators.compute()\n</pre> # Execute the training optimazed_estimators_realized = optimazed_estimators.compute() In\u00a0[13]: Copied! <pre># To be replaced by \"modelstore\"\nimport pickle\n\nfor model, id in zip(optimazed_estimators_realized, range(len(optimazed_estimators_realized))):\n    name_model = f\"model{id}.pickle\"\n    with open(name_model, \"wb\") as f:\n        pickle.dump(model, f)\n</pre> # To be replaced by \"modelstore\" import pickle  for model, id in zip(optimazed_estimators_realized, range(len(optimazed_estimators_realized))):     name_model = f\"model{id}.pickle\"     with open(name_model, \"wb\") as f:         pickle.dump(model, f) In\u00a0[14]: Copied! <pre># Load the models back\nlist_model = []\nfor id in range(5): \n    with open(f\"model{id}.pickle\", \"rb\") as f:\n        list_model.append(pickle.load(f))\nlist_model\n</pre> # Load the models back list_model = [] for id in range(5):      with open(f\"model{id}.pickle\", \"rb\") as f:         list_model.append(pickle.load(f)) list_model Out[14]: <pre>[GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error'])]</pre> In\u00a0[15]: Copied! <pre>from sklearn.metrics import mean_squared_error , r2_score,  mean_absolute_error\n\n# This for need to be coverted to a user defined \nlist_metrics = []\nfor model, test_data in zip(list_model, test_bags.compute()):\n    test_data = test_data.dropna()\n    X_test = test_data[input_list]\n    Y_test = test_data[output_list]\n    Y_eval = model.predict(X_test)\n\n    metrics = {\"MSE_SVR\": mean_squared_error(Y_test,Y_eval),\n               \"MAE_SVR\": mean_absolute_error(Y_test,Y_eval),\n               \"R_2\":r2_score(Y_test,Y_eval)}\n    list_metrics.append(metrics)\n</pre> from sklearn.metrics import mean_squared_error , r2_score,  mean_absolute_error  # This for need to be coverted to a user defined  list_metrics = [] for model, test_data in zip(list_model, test_bags.compute()):     test_data = test_data.dropna()     X_test = test_data[input_list]     Y_test = test_data[output_list]     Y_eval = model.predict(X_test)      metrics = {\"MSE_SVR\": mean_squared_error(Y_test,Y_eval),                \"MAE_SVR\": mean_absolute_error(Y_test,Y_eval),                \"R_2\":r2_score(Y_test,Y_eval)}     list_metrics.append(metrics)"},{"location":"notebooks/example_daskml/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/example_daskml/#split-per-gridcell","title":"Split per gridcell\u00b6","text":""},{"location":"notebooks/example_daskml/#train-test-split","title":"Train Test Split\u00b6","text":""},{"location":"notebooks/example_daskml/#setup-training","title":"Setup Training\u00b6","text":""},{"location":"notebooks/example_daskml/#model-optimization","title":"Model Optimization\u00b6","text":""},{"location":"notebooks/example_daskml/#save-model","title":"Save model\u00b6","text":""},{"location":"notebooks/example_daskml/#model-performance-evaluation","title":"Model performance evaluation\u00b6","text":""},{"location":"notebooks/example_dnn/","title":"Prallely training DNN with Tensorflow","text":"<p>This notebooks demonstrate how to split data to train-test execute parallel DNN trainings.</p> <p>The example dataset <code>./example1_data.zarr/</code> can be generated using this Jupyter Notebook.</p> In\u00a0[15]: Copied! <pre>import xarray as xr\nimport motrainer\nimport dask_ml.model_selection as dcv\nfrom motrainer.jackknife import JackknifeGPI\n</pre> import xarray as xr import motrainer import dask_ml.model_selection as dcv from motrainer.jackknife import JackknifeGPI In\u00a0[16]: Copied! <pre># Read the data\nzarr_file_path = \"./example1_data.zarr\"\nds = xr.open_zarr(zarr_file_path)\n</pre> # Read the data zarr_file_path = \"./example1_data.zarr\" ds = xr.open_zarr(zarr_file_path) In\u00a0[17]: Copied! <pre>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n    \nbags = motrainer.dataset_split(ds, \"space\")\nbags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)\n\ntest_size = 0.33\nf_shuffle = True\ntrain_test_bags = bags.map(\n    dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1\n)  \ntrain_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</pre> def to_dataframe(ds):     return ds.to_dask_dataframe()  def chunk(ds, chunks):     return ds.chunk(chunks)      bags = motrainer.dataset_split(ds, \"space\") bags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)  test_size = 0.33 f_shuffle = True train_test_bags = bags.map(     dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1 )   train_bags = train_test_bags.pluck(0) test_bags = train_test_bags.pluck(1) In\u00a0[18]: Copied! <pre># JackKnife parameters\nJackKnife = {\n    'val_split_year': 2017,\n    'output_list': ['sig', 'slop', 'curv'],\n    'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],\n    'out_path': './dnn_examples/results'\n}\n\n# Training parameters\nsearching_space = {\n    'num_dense_layers': [1, 10],\n    'num_input_nodes': [1, 6],\n    'num_dense_nodes': [1, 128],\n    'learning_rate': [5e-4, 1e-2],\n    'activation': ['relu']\n}\n\n# Here, I reduce parameters to be able to run on my own machine\noptimize_space = {\n    'best_loss': 2, # 1\n    'n_calls': 11, # 15\n    'epochs': 5, # 300\n    'noise': 0.1, \n    'kappa': 5,\n    'validation_split': 0.2,\n    'x0': [1e-3, 1, 4, 13, 'relu', 64]\n} # For weightling loss: 'loss_weights': [1, 1, 0.5],\n</pre> # JackKnife parameters JackKnife = {     'val_split_year': 2017,     'output_list': ['sig', 'slop', 'curv'],     'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],     'out_path': './dnn_examples/results' }  # Training parameters searching_space = {     'num_dense_layers': [1, 10],     'num_input_nodes': [1, 6],     'num_dense_nodes': [1, 128],     'learning_rate': [5e-4, 1e-2],     'activation': ['relu'] }  # Here, I reduce parameters to be able to run on my own machine optimize_space = {     'best_loss': 2, # 1     'n_calls': 11, # 15     'epochs': 5, # 300     'noise': 0.1,      'kappa': 5,     'validation_split': 0.2,     'x0': [1e-3, 1, 4, 13, 'relu', 64] } # For weightling loss: 'loss_weights': [1, 1, 0.5],  In\u00a0[19]: Copied! <pre># a function for training\ndef training_func(gpi_num, df, JackKnife, searching_space, optimize_space):\n    \n    # remove NA data\n    gpi_data = df.compute()\n    gpi_data.dropna(inplace=True)\n\n    # add time to index\n    gpi_data.set_index(\"time\", inplace=True, drop=True)\n\n    gpi = JackknifeGPI(gpi_data,\n                       JackKnife['val_split_year'],\n                       JackKnife['input_list'],\n                       JackKnife['output_list'],\n                       outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")\n\n    gpi.train(searching_space=searching_space,\n              optimize_space=optimize_space,\n              normalize_method='standard',\n              training_method='dnn',\n              performance_method='rmse',\n              verbose=2)\n\n    gpi.export_best()\n\n    return gpi.apr_perf, gpi.post_perf\n</pre> # a function for training def training_func(gpi_num, df, JackKnife, searching_space, optimize_space):          # remove NA data     gpi_data = df.compute()     gpi_data.dropna(inplace=True)      # add time to index     gpi_data.set_index(\"time\", inplace=True, drop=True)      gpi = JackknifeGPI(gpi_data,                        JackKnife['val_split_year'],                        JackKnife['input_list'],                        JackKnife['output_list'],                        outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")      gpi.train(searching_space=searching_space,               optimize_space=optimize_space,               normalize_method='standard',               training_method='dnn',               performance_method='rmse',               verbose=2)      gpi.export_best()      return gpi.apr_perf, gpi.post_perf <p>By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the Dask Documentation.</p> In\u00a0[20]: Copied! <pre>from dask.distributed import Client\n\nclient = Client()\n</pre> from dask.distributed import Client  client = Client() In\u00a0[21]: Copied! <pre>from dask.distributed import wait\n</pre> from dask.distributed import wait In\u00a0[23]: Copied! <pre># Use client to parallelize the loop across workers\nfutures = [\n    client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags)\n]\n\n# Wait for all computations to finish\nwait(futures)\n\n# Get the results\nresults = client.gather(futures)\n</pre> # Use client to parallelize the loop across workers futures = [     client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags) ]  # Wait for all computations to finish wait(futures)  # Get the results results = client.gather(futures) In\u00a0[32]: Copied! <pre># Close the Dask client\nclient.close()\n</pre> # Close the Dask client client.close() In\u00a0[31]: Copied! <pre># print the results\nfor gpi_num, performance in enumerate(results):\n    print(f\"GPI {(gpi_num + 1)}\")\n    print(\" aprior performance(RMSE):\")\n    print(performance[0])\n    print(\"post performance(RMSE):\")\n    print(performance[1])\n    print(\"=========================================\")\n</pre> # print the results for gpi_num, performance in enumerate(results):     print(f\"GPI {(gpi_num + 1)}\")     print(\" aprior performance(RMSE):\")     print(performance[0])     print(\"post performance(RMSE):\")     print(performance[1])     print(\"=========================================\") <pre>GPI 1\n aprior performance(RMSE):\n[[0.26438]\n [0.03295]\n [0.14362]]\npost performance(RMSE):\n[[0.34483]\n [0.00631]\n [0.00686]]\n=========================================\nGPI 2\n aprior performance(RMSE):\n[[0.37801]\n [0.02598]\n [0.26245]]\npost performance(RMSE):\n[[0.70249]\n [0.22075]\n [0.24423]]\n=========================================\nGPI 3\n aprior performance(RMSE):\n[[0.31875]\n [0.24323]\n [0.05353]]\npost performance(RMSE):\n[[0.03498]\n [0.19958]\n [0.24324]]\n=========================================\nGPI 4\n aprior performance(RMSE):\n[[0.19431]\n [0.10026]\n [0.16398]]\npost performance(RMSE):\n[[0.20526]\n [0.02813]\n [0.21003]]\n=========================================\nGPI 5\n aprior performance(RMSE):\n[[0.23724]\n [0.1104 ]\n [0.28052]]\npost performance(RMSE):\n[[0.10751]\n [0.08874]\n [0.26091]]\n=========================================\n</pre> <p>Shutdown the client to free up the resources click on SHUTDOWN in the Dask JupyterLab extension.</p>"},{"location":"notebooks/example_dnn/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"notebooks/example_dnn/#read-data-and-split-to-train-and-test-datasets","title":"Read data and split to train and test datasets\u00b6","text":""},{"location":"notebooks/example_dnn/#define-training-parameters","title":"Define training parameters\u00b6","text":""},{"location":"notebooks/example_dnn/#run-the-training","title":"Run the training\u00b6","text":"<p>In this example, we will demonstrate how to run the training parralel per grid (partition) with a dask cluster.</p>"},{"location":"notebooks/example_read_from_multiple_df/","title":"Align multiple DataFrames to a Dataset","text":"<p>An example notebook that reads data and convert it to xarray Dataset with aligned time and space axis.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt # only for plots\nfrom pathlib import Path\n</pre> import pandas as pd import xarray as xr import matplotlib.pyplot as plt # only for plots from pathlib import Path In\u00a0[2]: Copied! <pre>data_dir = \"./example_data/dssat_s1/\"\nnc_file_path = \"./example2_data.nc\"\nzarr_file_path = \"./example2_data.zarr\"\n</pre> data_dir = \"./example_data/dssat_s1/\" nc_file_path = \"./example2_data.nc\" zarr_file_path = \"./example2_data.zarr\" In\u00a0[3]: Copied! <pre>pickle_files_path = list(Path(data_dir).rglob('*.pkl'))\npickle_files_path\n</pre> pickle_files_path = list(Path(data_dir).rglob('*.pkl')) pickle_files_path Out[3]: <pre>[PosixPath('example_data/dssat_s1/DSSAT/brabant_LAI.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_CWAD.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_SWTD.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_SWTD6.pkl'),\n PosixPath('example_data/dssat_s1/Sentinel-1/Amp_CR_New.pkl')]</pre> In\u00a0[4]: Copied! <pre>for file in pickle_files_path:\n    df = pd.read_pickle(file)  # read file as pandas dataframe\n    print(file.stem)  # check file name\n    print(df.index.dtype)  # check index type\n    print(len(df.index))  # check length of index\n    print(len(df.columns))  # check length of columns\n    print(\"********\")\n</pre> for file in pickle_files_path:     df = pd.read_pickle(file)  # read file as pandas dataframe     print(file.stem)  # check file name     print(df.index.dtype)  # check index type     print(len(df.index))  # check length of index     print(len(df.columns))  # check length of columns     print(\"********\") <pre>brabant_LAI\ndatetime64[ns]\n148\n1283\n********\nbrabant_CWAD\ndatetime64[ns]\n148\n1283\n********\nbrabant_SWTD\ndatetime64[ns]\n274\n1283\n********\nbrabant_SWTD6\ndatetime64[ns]\n274\n1283\n********\nAmp_CR_New\nobject\n60\n21927\n********\n</pre> In\u00a0[5]: Copied! <pre># Read the data\nds_list = []\nfor file in pickle_files_path:\n    # read files and extract filename\n    df = pd.read_pickle(file)\n    var_name = file.stem\n\n    # Check the dtype of the index. If it's not datetime64[ns], convert it.\n    if df.index.dtype != \"datetime64[ns]\":\n        df.index = pd.to_datetime(df.index)\n\n    # convert dataframe to dataset\n    ds = xr.Dataset({var_name: ([\"time\", \"space\"], df.values)},\n                     coords={\"time\": df.index, \"space\": df.columns})\n    ds_list.append(ds)\n\n# Create one dataset\ndataset = xr.concat(ds_list, dim=\"time\")\n\n# Add attribute (metadata)\ndataset.attrs['source'] = 'data source'\ndataset.attrs['license'] = 'data license'\n</pre> # Read the data ds_list = [] for file in pickle_files_path:     # read files and extract filename     df = pd.read_pickle(file)     var_name = file.stem      # Check the dtype of the index. If it's not datetime64[ns], convert it.     if df.index.dtype != \"datetime64[ns]\":         df.index = pd.to_datetime(df.index)      # convert dataframe to dataset     ds = xr.Dataset({var_name: ([\"time\", \"space\"], df.values)},                      coords={\"time\": df.index, \"space\": df.columns})     ds_list.append(ds)  # Create one dataset dataset = xr.concat(ds_list, dim=\"time\")  # Add attribute (metadata) dataset.attrs['source'] = 'data source' dataset.attrs['license'] = 'data license' In\u00a0[6]: Copied! <pre>dataset\n</pre> dataset Out[6]: <pre>&lt;xarray.Dataset&gt;\nDimensions:        (space: 21927, time: 904)\nCoordinates:\n  * space          (space) int64 1526157 1526199 1526200 ... 2311808 2311820\n  * time           (time) datetime64[ns] 2017-05-06 2017-05-07 ... 2017-12-27\nData variables:\n    brabant_LAI    (time, space) float64 0.0 0.0 0.0 0.0 0.0 ... nan nan nan nan\n    brabant_CWAD   (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    brabant_SWTD   (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    brabant_SWTD6  (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    Amp_CR_New     (time, space) float64 nan nan nan nan ... 0.445 0.3501 0.3075\nAttributes:\n    source:   data source\n    license:  data license</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>space: 21927</li><li>time: 904</li></ul></li><li>Coordinates: (2)<ul><li>space(space)int641526157 1526199 ... 2311808 2311820<pre>array([1526157, 1526199, 1526200, ..., 2311765, 2311808, 2311820])</pre></li><li>time(time)datetime64[ns]2017-05-06 ... 2017-12-27<pre>array(['2017-05-06T00:00:00.000000000', '2017-05-07T00:00:00.000000000',\n       '2017-05-08T00:00:00.000000000', ..., '2017-12-15T00:00:00.000000000',\n       '2017-12-21T00:00:00.000000000', '2017-12-27T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (5)<ul><li>brabant_LAI(time, space)float640.0 0.0 0.0 0.0 ... nan nan nan nan<pre>array([[ 0.,  0.,  0., ..., nan, nan, nan],\n       [ 0.,  0.,  0., ..., nan, nan, nan],\n       [ 0.,  0.,  0., ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_CWAD(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_SWTD(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_SWTD6(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>Amp_CR_New(time, space)float64nan nan nan ... 0.445 0.3501 0.3075<pre>array([[       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       ...,\n       [0.14172222, 0.2719292 , 0.24513262, ..., 0.3101958 , 0.28541936,\n        0.22427617],\n       [0.20507831, 0.32489699, 0.31356163, ..., 0.35787517, 0.32660959,\n        0.29032968],\n       [0.19860778, 0.27824248, 0.27584513, ..., 0.4449813 , 0.35008485,\n        0.30750795]])</pre></li></ul></li><li>Indexes: (2)<ul><li>spacePandasIndex<pre>PandasIndex(Int64Index([1526157, 1526199, 1526200, 1526234, 1526249, 1526374, 1526375,\n            1526429, 1526436, 1526442,\n            ...\n            2311568, 2311595, 2311608, 2311669, 2311688, 2311706, 2311743,\n            2311765, 2311808, 2311820],\n           dtype='int64', name='space', length=21927))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2017-05-06', '2017-05-07', '2017-05-08', '2017-05-09',\n               '2017-05-10', '2017-05-11', '2017-05-12', '2017-05-13',\n               '2017-05-14', '2017-05-15',\n               ...\n               '2017-11-03', '2017-11-09', '2017-11-15', '2017-11-21',\n               '2017-11-27', '2017-12-03', '2017-12-09', '2017-12-15',\n               '2017-12-21', '2017-12-27'],\n              dtype='datetime64[ns]', name='time', length=904, freq=None))</pre></li></ul></li><li>Attributes: (2)source :data sourcelicense :data license</li></ul> In\u00a0[7]: Copied! <pre># Time series of one variable at one location\nAmp_CR_New = dataset.Amp_CR_New.isel(space=0)\nAmp_CR_New.plot()\n</pre> # Time series of one variable at one location Amp_CR_New = dataset.Amp_CR_New.isel(space=0) Amp_CR_New.plot() Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f28a87059d0&gt;]</pre> In\u00a0[8]: Copied! <pre># Save data in netcdf format\ndataset.to_netcdf(nc_file_path)\n</pre> # Save data in netcdf format dataset.to_netcdf(nc_file_path) In\u00a0[9]: Copied! <pre># For large dataset, chunk the data and save data in zarr format\ndataset.chunk({'space':1000})\ndataset.to_zarr(zarr_file_path)\n</pre> # For large dataset, chunk the data and save data in zarr format dataset.chunk({'space':1000}) dataset.to_zarr(zarr_file_path) Out[9]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f28909af8b0&gt;</pre>"},{"location":"notebooks/example_read_from_multiple_df/#import-libraries-and-set-paths","title":"Import libraries and set paths\u00b6","text":""},{"location":"notebooks/example_read_from_multiple_df/#read-the-data-and-explore-it","title":"Read the data and explore it\u00b6","text":""},{"location":"notebooks/example_read_from_multiple_df/#convert-data","title":"Convert data\u00b6","text":"<p>As seen above, the type of index of data are not consitent among different files e.g. \"object\" and \"datetime64[ns]\". Below, we convert it to the same type.</p>"},{"location":"notebooks/example_read_from_multiple_df/#inspect-output-and-store-it","title":"Inspect output and store it\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/","title":"Covert a nested DataFrame to a Dataset","text":"<p>An example notebook that reads data and convert it to xarray Dataset with aligned time and space axis.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt # only for plots\n</pre> import pandas as pd import xarray as xr import matplotlib.pyplot as plt # only for plots In\u00a0[2]: Copied! <pre>pickle_file_path = \"./example_data/example_data.pickle\"\nnc_file_path = \"./example1_data.nc\"\nzarr_file_path = \"./example1_data.zarr\"\n</pre> pickle_file_path = \"./example_data/example_data.pickle\" nc_file_path = \"./example1_data.nc\" zarr_file_path = \"./example1_data.zarr\" In\u00a0[3]: Copied! <pre># Read the data\ndf_all_gpi = pd.read_pickle(pickle_file_path)\n</pre> # Read the data df_all_gpi = pd.read_pickle(pickle_file_path) In\u00a0[4]: Copied! <pre>df_all_gpi\n</pre> df_all_gpi Out[4]: lat lon data 1 56.125 11.375 sig      slop      curv    ... 2 46.125 6.625 sig      slop      curv    ... 3 53.375 6.125 sig      slop      curv    ... 4 49.375 12.375 sig      slop      curv    ... 5 44.375 0.625 sig      slop      curv    ... In\u00a0[5]: Copied! <pre>df_all_gpi.iloc[3][\"data\"]\n</pre> df_all_gpi.iloc[3][\"data\"] Out[5]: sig slop curv TG1 TG2 TG3 WG1 WG2 WG3 BIOMA1 BIOMA2 datetime_doy 2007-01-02 -8.774847 -0.118061 -0.001871 282.495667 277.571790 280.432019 0.353169 0.297954 0.316928 0.055779 0.064610 2007-01-03 -8.737255 -0.116761 -0.001753 283.059404 278.609833 279.851678 0.224477 0.336283 0.303121 0.057188 0.007182 2007-01-03 -8.791911 -0.118357 -0.002037 284.386143 278.075722 285.383157 0.378645 0.250349 0.335715 0.062280 0.043909 2007-01-05 -7.962205 -0.118063 -0.002072 276.947048 277.841682 277.941320 0.305945 0.332280 0.315607 0.052877 0.017596 2007-01-06 -8.607216 -0.118727 -0.002048 276.458553 282.783491 277.956962 0.380480 0.364697 0.280530 0.051309 0.034444 ... ... ... ... ... ... ... ... ... ... ... ... 2019-12-30 -8.824627 -0.119621 -0.000872 NaN NaN NaN NaN NaN NaN NaN NaN 2019-12-31 -8.578708 -0.121446 -0.001059 NaN NaN NaN NaN NaN NaN NaN NaN 2019-12-31 -8.731547 -0.119538 -0.000887 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 -7.358630 -0.122284 -0.000725 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 -9.165778 -0.123732 -0.000753 NaN NaN NaN NaN NaN NaN NaN NaN <p>7995 rows \u00d7 11 columns</p> In\u00a0[6]: Copied! <pre># Function to make timestamps unique by adding half an hour\ndef make_timestamps_unique(df):\n    seen_timestamps = set()\n    new_index = []\n\n    for timestamp in df.index:\n        if timestamp not in seen_timestamps:\n            new_index.append(timestamp)\n            seen_timestamps.add(timestamp)\n        else:\n            # Timestamp is a duplicate, add half an hour\n            while timestamp in seen_timestamps:\n                timestamp += pd.Timedelta(minutes=30)\n            new_index.append(timestamp)\n            seen_timestamps.add(timestamp)\n    \n    df.index = new_index\n    df.index.name = \"time\"\n    return df\n</pre> # Function to make timestamps unique by adding half an hour def make_timestamps_unique(df):     seen_timestamps = set()     new_index = []      for timestamp in df.index:         if timestamp not in seen_timestamps:             new_index.append(timestamp)             seen_timestamps.add(timestamp)         else:             # Timestamp is a duplicate, add half an hour             while timestamp in seen_timestamps:                 timestamp += pd.Timedelta(minutes=30)             new_index.append(timestamp)             seen_timestamps.add(timestamp)          df.index = new_index     df.index.name = \"time\"     return df In\u00a0[7]: Copied! <pre>ds_list = []\nfor index, row in df_all_gpi.iterrows():\n    \n    # Filter the nested DataFrame based on location\n    df = df_all_gpi.iloc[index-1][\"data\"]\n\n    # Make timestamps unique\n    df = make_timestamps_unique(df)\n\n    # convert dataframe to dataset\n    ds = xr.Dataset(df, coords={'latitude': row[\"lat\"], 'longitude': row[\"lon\"]})\n    ds_list.append(ds)\n\n# Create one dataset\ndataset = xr.concat(ds_list, dim=\"space\")\n\n# Add attribute (metadata)\ndataset.attrs['source'] = 'data source'\ndataset.attrs['license'] = 'data license'\n</pre> ds_list = [] for index, row in df_all_gpi.iterrows():          # Filter the nested DataFrame based on location     df = df_all_gpi.iloc[index-1][\"data\"]      # Make timestamps unique     df = make_timestamps_unique(df)      # convert dataframe to dataset     ds = xr.Dataset(df, coords={'latitude': row[\"lat\"], 'longitude': row[\"lon\"]})     ds_list.append(ds)  # Create one dataset dataset = xr.concat(ds_list, dim=\"space\")  # Add attribute (metadata) dataset.attrs['source'] = 'data source' dataset.attrs['license'] = 'data license' In\u00a0[8]: Copied! <pre>dataset\n</pre> dataset Out[8]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (time: 8506, space: 5)\nCoordinates:\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\n    latitude   (space) float64 56.12 46.12 53.38 49.38 44.38\n    longitude  (space) float64 11.38 6.625 6.125 12.38 0.625\nDimensions without coordinates: space\nData variables:\n    sig        (space, time) float64 -9.49 -8.494 -9.069 ... -8.071 -8.237\n    slop       (space, time) float64 -0.1208 -0.1178 -0.121 ... -0.1144 -0.1191\n    curv       (space, time) float64 -0.001396 -0.001464 ... -0.0006173\n    TG1        (space, time) float64 280.0 270.4 285.5 277.4 ... nan nan nan nan\n    TG2        (space, time) float64 274.8 278.4 280.6 283.7 ... nan nan nan nan\n    TG3        (space, time) float64 280.9 279.7 278.0 278.0 ... nan nan nan nan\n    WG1        (space, time) float64 0.3249 0.2798 0.2773 0.2867 ... nan nan nan\n    WG2        (space, time) float64 0.3408 0.2902 0.3373 0.2709 ... nan nan nan\n    WG3        (space, time) float64 0.3123 0.2916 0.2891 0.3538 ... nan nan nan\n    BIOMA1     (space, time) float64 0.07079 0.05532 0.04846 ... nan nan nan\n    BIOMA2     (space, time) float64 0.04366 0.0462 0.03821 ... nan nan nan\nAttributes:\n    source:   data source\n    license:  data license</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 8506</li><li>space: 5</li></ul></li><li>Coordinates: (3)<ul><li>time(time)datetime64[ns]2007-01-02 ... 2020-01-01T01:00:00<pre>array(['2007-01-02T00:00:00.000000000', '2007-01-03T00:00:00.000000000',\n       '2007-01-03T00:30:00.000000000', ..., '2020-01-01T00:00:00.000000000',\n       '2020-01-01T00:30:00.000000000', '2020-01-01T01:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>latitude(space)float6456.12 46.12 53.38 49.38 44.38<pre>array([56.125, 46.125, 53.375, 49.375, 44.375])</pre></li><li>longitude(space)float6411.38 6.625 6.125 12.38 0.625<pre>array([11.375,  6.625,  6.125, 12.375,  0.625])</pre></li></ul></li><li>Data variables: (11)<ul><li>sig(space, time)float64-9.49 -8.494 ... -8.071 -8.237<pre>array([[-9.48985745, -8.49424829, -9.06917897, ..., -8.87350746,\n        -8.55944322,         nan],\n       [-9.02591086, -8.79194073, -8.24172421, ..., -8.65639331,\n        -7.39492538, -8.74562845],\n       [-9.29394782, -8.87621932, -8.96752995, ..., -8.2149011 ,\n        -9.08360639,         nan],\n       [-8.77484736, -8.73725534, -8.79191062, ..., -7.35863039,\n        -9.16577772,         nan],\n       [-8.24819331, -8.8397918 , -9.10849921, ..., -8.06904894,\n        -8.07118479, -8.23718386]])</pre></li><li>slop(space, time)float64-0.1208 -0.1178 ... -0.1144 -0.1191<pre>array([[-0.12082425, -0.11783219, -0.12098574, ..., -0.11359581,\n        -0.11493786,         nan],\n       [-0.11527791, -0.11782626, -0.11479579, ..., -0.12395586,\n        -0.12741118, -0.12636578],\n       [-0.11198183, -0.11500686, -0.11297782, ..., -0.11395541,\n        -0.1149019 ,         nan],\n       [-0.11806147, -0.11676077, -0.11835654, ..., -0.1222841 ,\n        -0.1237319 ,         nan],\n       [-0.11225743, -0.1164341 , -0.11506161, ..., -0.11670426,\n        -0.114442  , -0.11907956]])</pre></li><li>curv(space, time)float64-0.001396 -0.001464 ... -0.0006173<pre>array([[-0.00139633, -0.00146407, -0.00155133, ..., -0.00147185,\n        -0.00133919,         nan],\n       [-0.00192987, -0.00172127, -0.00155783, ..., -0.00127703,\n        -0.0010599 , -0.00104103],\n       [-0.00175796, -0.00185267, -0.0018512 , ..., -0.00093651,\n        -0.00089887,         nan],\n       [-0.0018711 , -0.00175281, -0.00203733, ..., -0.00072492,\n        -0.00075293,         nan],\n       [-0.00105648, -0.00112672, -0.0011477 , ..., -0.00039547,\n        -0.00081939, -0.00061725]])</pre></li><li>TG1(space, time)float64280.0 270.4 285.5 ... nan nan nan<pre>array([[280.02112326, 270.43043758, 285.49255157, ...,          nan,\n                 nan,          nan],\n       [273.95243199, 279.10472356, 275.67491766, ...,          nan,\n                 nan,          nan],\n       [282.79039682, 285.25875506, 286.80482212, ...,          nan,\n                 nan,          nan],\n       [282.49566693, 283.05940391, 284.38614309, ...,          nan,\n                 nan,          nan],\n       [272.8261471 , 280.37911393, 283.709762  , ...,          nan,\n                 nan,          nan]])</pre></li><li>TG2(space, time)float64274.8 278.4 280.6 ... nan nan nan<pre>array([[274.84709093, 278.41673812, 280.57289091, ...,          nan,\n                 nan,          nan],\n       [278.42437536, 283.31915753, 278.51042955, ...,          nan,\n                 nan,          nan],\n       [279.21544423, 279.57413371, 284.76804529, ...,          nan,\n                 nan,          nan],\n       [277.57179007, 278.6098334 , 278.07572192, ...,          nan,\n                 nan,          nan],\n       [278.54961327, 283.62250125, 286.60718795, ...,          nan,\n                 nan,          nan]])</pre></li><li>TG3(space, time)float64280.9 279.7 278.0 ... nan nan nan<pre>array([[280.94967399, 279.73701754, 277.95140294, ...,          nan,\n                 nan,          nan],\n       [275.7377006 , 278.96455422, 283.03484774, ...,          nan,\n                 nan,          nan],\n       [277.36359601, 279.51634776, 281.45332132, ...,          nan,\n                 nan,          nan],\n       [280.43201923, 279.85167842, 285.38315711, ...,          nan,\n                 nan,          nan],\n       [280.93250698, 277.34480673, 276.23607919, ...,          nan,\n                 nan,          nan]])</pre></li><li>WG1(space, time)float640.3249 0.2798 0.2773 ... nan nan<pre>array([[0.32492937, 0.27983456, 0.27729615, ...,        nan,        nan,\n               nan],\n       [0.28419109, 0.29665461, 0.26357131, ...,        nan,        nan,\n               nan],\n       [0.28657149, 0.27418057, 0.32130955, ...,        nan,        nan,\n               nan],\n       [0.35316888, 0.22447731, 0.37864479, ...,        nan,        nan,\n               nan],\n       [0.32013111, 0.32989414, 0.37377789, ...,        nan,        nan,\n               nan]])</pre></li><li>WG2(space, time)float640.3408 0.2902 0.3373 ... nan nan<pre>array([[0.34075338, 0.29023887, 0.33732519, ...,        nan,        nan,\n               nan],\n       [0.29739347, 0.31481947, 0.32647732, ...,        nan,        nan,\n               nan],\n       [0.31802497, 0.35188802, 0.27310549, ...,        nan,        nan,\n               nan],\n       [0.29795442, 0.33628332, 0.25034937, ...,        nan,        nan,\n               nan],\n       [0.34499846, 0.3227702 , 0.34632033, ...,        nan,        nan,\n               nan]])</pre></li><li>WG3(space, time)float640.3123 0.2916 0.2891 ... nan nan<pre>array([[0.31225323, 0.29158948, 0.28909846, ...,        nan,        nan,\n               nan],\n       [0.31240911, 0.28942008, 0.28934139, ...,        nan,        nan,\n               nan],\n       [0.31223442, 0.32122713, 0.25863411, ...,        nan,        nan,\n               nan],\n       [0.31692828, 0.30312131, 0.33571518, ...,        nan,        nan,\n               nan],\n       [0.34937695, 0.33768108, 0.3156577 , ...,        nan,        nan,\n               nan]])</pre></li><li>BIOMA1(space, time)float640.07079 0.05532 0.04846 ... nan nan<pre>array([[0.07078687, 0.05531632, 0.04845626, ...,        nan,        nan,\n               nan],\n       [0.06313461, 0.06849046, 0.06948795, ...,        nan,        nan,\n               nan],\n       [0.10973646, 0.09282028, 0.10490551, ...,        nan,        nan,\n               nan],\n       [0.05577876, 0.05718846, 0.06228029, ...,        nan,        nan,\n               nan],\n       [0.06641911, 0.07987195, 0.07466871, ...,        nan,        nan,\n               nan]])</pre></li><li>BIOMA2(space, time)float640.04366 0.0462 0.03821 ... nan nan<pre>array([[0.04365577, 0.04620425, 0.03821127, ...,        nan,        nan,\n               nan],\n       [0.05440495, 0.03978223, 0.00253587, ...,        nan,        nan,\n               nan],\n       [0.11810849, 0.13832198, 0.1520621 , ...,        nan,        nan,\n               nan],\n       [0.06460992, 0.00718201, 0.04390921, ...,        nan,        nan,\n               nan],\n       [0.04191187, 0.04335878, 0.08054077, ...,        nan,        nan,\n               nan]])</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2007-01-02 00:00:00', '2007-01-03 00:00:00',\n               '2007-01-03 00:30:00', '2007-01-05 00:00:00',\n               '2007-01-06 00:00:00', '2007-01-08 00:00:00',\n               '2007-01-08 00:30:00', '2007-01-09 00:00:00',\n               '2007-01-10 00:00:00', '2007-01-11 00:00:00',\n               ...\n               '2019-12-29 01:00:00', '2019-12-30 00:00:00',\n               '2019-12-30 00:30:00', '2019-12-30 01:00:00',\n               '2019-12-30 01:30:00', '2019-12-31 00:00:00',\n               '2019-12-31 00:30:00', '2020-01-01 00:00:00',\n               '2020-01-01 00:30:00', '2020-01-01 01:00:00'],\n              dtype='datetime64[ns]', name='time', length=8506, freq=None))</pre></li></ul></li><li>Attributes: (2)source :data sourcelicense :data license</li></ul> In\u00a0[9]: Copied! <pre># Time series of one variable at one location\nsig = dataset.sig.isel(space=0)\nsig.plot()\n</pre> # Time series of one variable at one location sig = dataset.sig.isel(space=0) sig.plot() Out[9]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fe3818fb510&gt;]</pre> In\u00a0[10]: Copied! <pre># map of one variable at one time\ntime = \"2007-01-02T00:00:00.000000000\"\nsig = dataset.sig.sel(time=time)\nlons, lats = xr.broadcast(sig.longitude, sig.latitude)\nplt.figure(figsize=(8, 6))\nplt.scatter(lons, lats, c=sig, cmap=\"viridis\", marker=\"o\", s=10)\nplt.colorbar(label=\"sig\") \nplt.xlabel(\"longitude\")\nplt.ylabel(\"latitude\")\nplt.title(f\"Plot of sig at time {time}\")\nplt.grid(True)\nplt.show()\n</pre> # map of one variable at one time time = \"2007-01-02T00:00:00.000000000\" sig = dataset.sig.sel(time=time) lons, lats = xr.broadcast(sig.longitude, sig.latitude) plt.figure(figsize=(8, 6)) plt.scatter(lons, lats, c=sig, cmap=\"viridis\", marker=\"o\", s=10) plt.colorbar(label=\"sig\")  plt.xlabel(\"longitude\") plt.ylabel(\"latitude\") plt.title(f\"Plot of sig at time {time}\") plt.grid(True) plt.show() In\u00a0[11]: Copied! <pre># Save data in netcdf format\ndataset.to_netcdf(nc_file_path)\n</pre> # Save data in netcdf format dataset.to_netcdf(nc_file_path) In\u00a0[12]: Copied! <pre># For large dataset, chunk the data and save data in zarr format\ndataset.chunk({'space':1000})\ndataset.to_zarr(zarr_file_path)\n</pre> # For large dataset, chunk the data and save data in zarr format dataset.chunk({'space':1000}) dataset.to_zarr(zarr_file_path) Out[12]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7fe2fffda420&gt;</pre>"},{"location":"notebooks/example_read_from_one_df/#import-libraries-and-set-paths","title":"Import libraries and set paths\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/#read-the-data-and-explore-it","title":"Read the data and explore it\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/#convert-data","title":"Convert data\u00b6","text":"<p>As seen above, the \"datetime_doy\" values are not unique. While it's possible to have non-unique index values, it's generally not recommended. Having a non-unique index can make certain operations and data manipulation more complex, or even incorrect. These values shows two observation at one day. To avoid duplication, we add a hal-hour shift.</p>"},{"location":"notebooks/example_read_from_one_df/#inspect-output-and-store-it","title":"Inspect output and store it\u00b6","text":""}]}