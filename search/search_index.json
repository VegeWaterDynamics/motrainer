{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MOTrainer","text":"<p>Measurement Operator Trainer (MOTrainer) is a Python package which facilitates parallel training processes of measurement operators (MO) for Data Assimilation (DA) purposes. It specifically address the need of parallely training multiple MOs over independent partitions of large spatio-temporal dataset.</p> <p>MOTrainer provices functionality to split spatio-temporal datasets for independent training processes. It utilizes Xarray's feature of multi-dimensional labelling to address the spatio-temporal characteres of the input/output datasets. Then Dask is implemented to achive the parallel training jobs.  </p> <p>MOTrainer supports training simple structured Machine Learning (ML) models which can be trained using SciKit-Learn toolkits. It also provide supports on parallel training Training DeepNeuron Networks with TensorFlow.</p>"},{"location":"CHANGELOG/","title":"Change Log","text":"<p>All notable changes to this project will be documented in this file. This project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#012-2023-01-11","title":"[0.1.2] - 2023-01-11","text":"<ul> <li>Added:<ul> <li>API documentation for major components.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011-2023-01-08","title":"[0.1.1] - 2023-01-08","text":"<ul> <li>Added:<ul> <li>Added model exportation examples to documentation.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#010-2023-12-21","title":"[0.1.0] - 2023-12-21","text":"<p>The first version of the MOTrainer package.</p> <ul> <li>Added:<ul> <li>Spatio temporal Dataset split;</li> <li>Jackknife GPI implementation;</li> <li>Documentation and examples for distributed training;</li> <li>Relevant tests.</li> </ul> </li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>This code of conduct is adapted from the  Git Code of Conduct.</p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at team-atlas@esciencecenter.nl.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All Project maintainers are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4,  available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"MOTrainer Contributing Guidelines","text":"<p>We welcome any kind of contribution to our software, from simple comment  or question to a full fledged pull request.  Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ul> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation).</li> </ul> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you-have-a-question","title":"You have a question","text":"<ul> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"question\" label; apply other labels when relevant.</li> </ul>"},{"location":"CONTRIBUTING/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ul> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>apply relevant labels to the newly created issue.</li> </ul>"},{"location":"CONTRIBUTING/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ul> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>make sure the existing tests still work. First, install the development dependencies as <code>pip install .[dev]</code>, and then run <code>pytest tests</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation. Make sure the documentation is built successfully: first, install documentation dependencies as <code>pip install .[docs]</code> and then run <code>mkdocs build</code>.</li> <li>make sure the linting tests pass by running <code>ruff</code> in the project root directory: <code>ruff check .</code>;</li> <li>push your feature branch to (your fork of) the MOTrainer repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ul> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>Here you find the api reference for the major components of <code>MOTrainer</code>.</p>"},{"location":"api_reference/#motrainersplitter","title":"motrainer.splitter:","text":""},{"location":"api_reference/#motrainer.splitter.dataset_split","title":"<code>dataset_split(ds, identifier)</code>","text":"<p>Split a Dataset by indentifier for independent training tasks.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Xarray Dataset to be splitted.</p> required <code>identifier</code> <code>dict | str</code> <p>When <code>indentifier</code> is a dictionary, its keys should be a subset of {\"space\", \"time\"},  and map \"space\" and/or \"time\" dimension with corresponding separation indentifier.</p> <p>When <code>indentifier</code> is a string, the separation will depends on if <code>indentifier</code> is a key of coords/data_vars or a dimension name (\"space\" or \"time\"). In the former case the corresponding coords/data_vars will be used as separation indentifier. In the latter case ds will be separated per entry in that dimension.</p> required <p>Returns:</p> Type Description <code>bag</code> <p>A Dask Databag of splited Datasets</p> Source code in <code>motrainer/splitter.py</code> <pre><code>def dataset_split(ds: xr.Dataset, identifier: dict | str) -&gt; db:\n    \"\"\"Split a Dataset by indentifier for independent training tasks.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Xarray Dataset to be splitted.\n    identifier : dict | str\n        When `indentifier` is a dictionary, its keys should be a subset of\n        {\"space\", \"time\"},  and map \"space\" and/or \"time\" dimension with corresponding\n        separation indentifier.\n\n        When `indentifier` is a string, the separation will depends on if `indentifier`\n        is a key of coords/data_vars or a dimension name (\"space\" or \"time\").\n        In the former case the corresponding coords/data_vars will be used as separation\n        indentifier.\n        In the latter case ds will be separated per entry in that dimension.\n\n    Returns\n    -------\n    dask.bag\n        A Dask Databag of splited Datasets\n    \"\"\"\n    identifier = _regulate_identifier(ds, identifier)\n\n    list_id = []\n    for key in MOT_DIMS:\n        if key in identifier.keys():\n            key_id = key + \"_id\"\n            ds = ds.assign_coords({key_id: (key, identifier[key])})\n            list_id.append(key_id)\n\n    # Get the name of attributes to groupby\n    if len(list_id) &gt; 1:\n        # Use space time cls coordinates as multi index\n        # Must stack on MOT_DIMS to reduce dims of data variable\n        multi_idx = ds.stack(samples=list_id).samples.values\n        for dim in MOT_DIMS:\n            if dim in ds.indexes:\n                ds = ds.reset_index(dim)\n        ds = (\n            ds.stack(\n                samples=MOT_DIMS, create_index=False\n            )  # No index creation since this will be added next.\n            .assign_coords(samples=multi_idx)\n            .set_xindex(list_id)\n        )\n        key_gb = \"samples\"\n    else:\n        key_gb = list_id[0]\n\n    # Groupby and separate to Dask Databags\n    list_db = []\n    for grp in list(ds.groupby(key_gb)):\n        list_db.append(grp[1])\n    bags = db.from_sequence(list_db)\n\n    return bags\n</code></pre>"},{"location":"api_reference/#motrainer.splitter.is_splitable","title":"<code>is_splitable(ds)</code>","text":"<p>Check if a Dataset is can be splitted using MOTrainer.</p> <p>The following checks will be applied:     - The Dastaset has exactly 2 dimensions     - The 2 dims are \"space\" and \"time\"     - There are no duplicated coordinates A UserWarning will be raised for each failed check.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Xarray Dataset to be splitted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Result of check in Boolean. If all checks are passed, it will be True. Otherwise False.</p> Source code in <code>motrainer/splitter.py</code> <pre><code>def is_splitable(ds: xr.Dataset) -&gt; bool:\n    \"\"\"Check if a Dataset is can be splitted using MOTrainer.\n\n    The following checks will be applied:\n        - The Dastaset has exactly 2 dimensions\n        - The 2 dims are \"space\" and \"time\"\n        - There are no duplicated coordinates\n    A UserWarning will be raised for each failed check.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Xarray Dataset to be splitted.\n\n    Returns\n    -------\n    bool\n       Result of check in Boolean.\n       If all checks are passed, it will be True. Otherwise False.\n    \"\"\"\n    flag_valid = True\n\n    # Dimension size should be 2\n    if len(ds.dims) != 2:\n        warnings.warn(\n            'Dataset should have two dimensions: \"space\" and \"time\"',\n            UserWarning,\n            stacklevel=2,\n        )\n        flag_valid = False\n\n    # space and time dimensions should exist\n    for dim in MOT_DIMS:\n        if dim not in ds.dims:\n            warnings.warn(\n                f\"{dim} not found in the dimensions\", UserWarning, stacklevel=2\n            )\n            flag_valid = False\n\n    # Check duplicated coordinates\n    for coord in ds.coords:\n        if np.unique(ds[coord]).shape != ds[coord].shape:\n            warnings.warn(\n                f\"Duplicated coordinates found in {coord}\", UserWarning, stacklevel=2\n            )\n            flag_valid = False\n\n    return flag_valid\n</code></pre>"},{"location":"api_reference/#motrainer.splitter.train_test_split","title":"<code>train_test_split(ds, mask=None, split=None, reverse=False)</code>","text":"<p>Split data to train and test datasets.</p> <p>The split is performed either 1) by specifying the training data mask (<code>mask</code>) where training data locations are True, or 2) by a specifying a coordinate value (<code>split</code>) splitting the data into two.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Xarray dataset to split</p> required <code>mask</code> <code>DataArray</code> <p>Mask, True at training data locations. By default None</p> <code>None</code> <code>split</code> <code>dict</code> <p>coordinate diactionary in {NAME: coordinates} which split the Dataset into two. The part smaller than it will be training, by default None.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Reverse the split results, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Dataset, Dataset]</code> <p>Split results. In (training, test).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When neither mask nor split is specified.</p> <code>ValueError</code> <p>When both mask and split are specified.</p> Source code in <code>motrainer/splitter.py</code> <pre><code>def train_test_split(\n    ds: xr.Dataset,\n    mask: xr.DataArray = None,\n    split: dict = None,\n    reverse: bool = False,\n) -&gt; tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"Split data to train and test datasets.\n\n    The split is performed either 1) by specifying the training data mask (`mask`)\n    where training data locations are True, or 2) by a specifying a coordinate value\n    (`split`) splitting the data into two.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Xarray dataset to split\n    mask : xr.DataArray, optional\n        Mask, True at training data locations. By default None\n    split : dict, optional\n        coordinate diactionary in {NAME: coordinates} which split the Dataset into two.\n        The part smaller than it will be training, by default None.\n    reverse : bool, optional\n        Reverse the split results, by default False\n\n    Returns\n    -------\n    tuple[xr.Dataset, xr.Dataset]\n        Split results. In (training, test).\n\n    Raises\n    ------\n    ValueError\n        When neither mask nor split is specified.\n    ValueError\n        When both mask and split are specified.\n    \"\"\"\n    if mask is None and split is None:\n        raise ValueError(\"Either mask or split should be specified.\")\n    elif mask is not None and split is not None:\n        raise ValueError(\"Only one of mask and split should be specified.\")\n\n    # Convert split to mask\n    if split is not None:\n        _validate_train_test_split(split)\n        mask = ds[list(split.keys())[0]] &lt; list(split.values())[0]\n\n    train = ds.where(mask, drop=True)\n    test = ds.where(~mask, drop=True)\n\n    return (test, train) if reverse else (train, test)\n</code></pre>"},{"location":"api_reference/#jackknifegpi","title":"JackknifeGPI:","text":""},{"location":"api_reference/#motrainer.jackknife.JackknifeGPI","title":"<code>motrainer.jackknife.JackknifeGPI(gpi_data, val_split_year, input_list, output_list, export_all_years=True, outpath='./jackknife_results')</code>","text":"<p>GPI object for neuron netowork training using Jackknife resampling method.</p> <p>Methods:</p> Name Description <code>train</code> <p>performance_method='rmse', training_method='dnn', verbose=0) train neuron network with given method</p> <code>export_best</code> <p>export the best results in Jackknife process.</p> <p>Initialize JackknifeGPI object.</p> <p>Parameters:</p> Name Type Description Default <code>gpi_data</code> <code>DataFrame</code> <p>DataFrame of a single GPI. Each row represents all properties at a certain timestamp. Each column represents a time-series of a property.</p> required <code>val_split_year</code> <code>int</code> <p>Split year of validation. All data after (include) this year will be reserved for benchmarking.</p> required <code>input_list</code> <code>list of str</code> <p>Column names in gpi_data will will be used as input.</p> required <code>output_list</code> <code>list of str</code> <p>Column names in gpi_data will will be used as output.</p> required <code>export_all_years</code> <code>bool</code> <p>Switch to export the results of all years, by default True</p> <code>True</code> <code>outpath</code> <code>str</code> <p>Results exporting path, by default './jackknife_results'</p> <code>'./jackknife_results'</code> Source code in <code>motrainer/jackknife.py</code> <pre><code>def __init__(self,\n             gpi_data,\n             val_split_year,\n             input_list,\n             output_list,\n             export_all_years=True,\n             outpath='./jackknife_results'):\n    \"\"\"Initialize JackknifeGPI object.\n\n    Parameters\n    ----------\n    gpi_data : pandas.DataFrame\n        DataFrame of a single GPI.\n        Each row represents all properties at a certain timestamp.\n        Each column represents a time-series of a property.\n    val_split_year : int\n        Split year of validation. All data after (include) this year will\n        be reserved for benchmarking.\n    input_list : list of str\n        Column names in gpi_data will will be used as input.\n    output_list : list of str\n        Column names in gpi_data will will be used as output.\n    export_all_years : bool, optional\n        Switch to export the results of all years, by default True\n    outpath : str, optional\n        Results exporting path, by default './jackknife_results'\n    \"\"\"\n    logger.info('Initializing Jackkinfe trainning:\\n'\n                f'val_split_year: {val_split_year}\\n'\n                f'input_list: {input_list}\\n'\n                f'output_list: {output_list}\\n')\n\n    assert not (\n        gpi_data.isnull().values.any()), 'Nan value(s) in gpi_data!'\n\n    self.gpi_data = gpi_data\n    self.input_list = input_list\n    self.output_list = output_list\n    self.gpi_input = gpi_data[input_list].copy()\n    self.gpi_output = gpi_data[output_list].copy()\n    self.val_split_year = val_split_year\n    self.export_all_years = export_all_years\n    self.outpath = outpath\n    Path(self.outpath).parent.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api_reference/#motrainer.jackknife.JackknifeGPI.export_best","title":"<code>export_best(model_name='best_optimized_model')</code>","text":"<p>Export the best results in Jackknife process.</p> Source code in <code>motrainer/jackknife.py</code> <pre><code>def export_best(self, model_name='best_optimized_model'):\n    \"\"\"Export the best results in Jackknife process.\"\"\"\n    logger.info(\n        'Exporting model and hyperparameters of '\n        f'year {self.best_year} to {self.outpath}'\n        )\n\n    if model_name is not None:\n        path_model = f'{self.outpath}/{model_name}_{self.best_year}'\n    else:\n        path_model = f'{self.outpath}/best_optimized_model_{self.best_year}'\n\n    # write metadata\n    metedata = {}\n    metedata['input_list'] = self.input_list\n    metedata['output_list'] = self.input_list\n    metedata['best_year'] = int(self.best_year)\n\n    for key in ['lat', 'lon', 'latitude', 'longitude']:\n        if key in self.gpi_data.keys():\n            metedata[key] = float(self.gpi_data[key].iloc[0])\n\n    self.best_train.export(path_model=path_model, meta_data=metedata)\n</code></pre>"},{"location":"api_reference/#motrainer.jackknife.JackknifeGPI.train","title":"<code>train(searching_space, optimize_space, normalize_method='standard', performance_method='rmse', training_method='dnn', verbose=0)</code>","text":"<p>Train neuron network with Jackknife resampling method.</p> <p>Procedures: 1. Reserve in/output after self.val_split_year for later benchmarking. 2. From the rest in/output data, leave out one year as validation data. 3. Perform neuron network training. 4. Repeat Step 2 and 3 until all years exept benchmarking years have     been used for validation. 5. Select the best trainning by best performance. 6. Perform benchmarking on reserved data.</p> <p>Parameters:</p> Name Type Description Default <code>searching_space</code> <code>dict</code> <p>Arguments of searching space.</p> required <code>optimize_space</code> <code>dict</code> <p>Arguments of optimazation space.</p> required <code>normalize_method</code> <code>str</code> <p>Method of normalization. Choose from 'standard' and 'min_max'. By default 'standard'</p> <code>'standard'</code> <code>performance_method</code> <code>str</code> <p>Method of computing performance. Choose from 'rmse', 'mae', 'pearson' and 'spearman'. By default 'rmse'.</p> <code>'rmse'</code> <code>training_method</code> <code>str</code> <p>Traning method selection. Select from 'dnn' or 'dnn_lossweights'. By default 'dnn'</p> <code>'dnn'</code> <code>verbose</code> <code>int</code> <p>Control the verbosity. By default 0, which means no screen feedback.</p> <code>0</code> Source code in <code>motrainer/jackknife.py</code> <pre><code>def train(self,\n          searching_space,\n          optimize_space,\n          normalize_method='standard',\n          performance_method='rmse',\n          training_method='dnn',\n          verbose=0):\n    \"\"\"Train neuron network with Jackknife resampling method.\n\n    Procedures:\n    1. Reserve in/output after self.val_split_year for later benchmarking.\n    2. From the rest in/output data, leave out one year as validation data.\n    3. Perform neuron network training.\n    4. Repeat Step 2 and 3 until all years exept benchmarking years have\n        been used for validation.\n    5. Select the best trainning by best performance.\n    6. Perform benchmarking on reserved data.\n\n    Parameters\n    ----------\n    searching_space : dict\n        Arguments of searching space.\n    optimize_space : dict\n        Arguments of optimazation space.\n    normalize_method : str, optional\n        Method of normalization. Choose from 'standard' and 'min_max'.\n        By default 'standard'\n    performance_method : str, optional\n        Method of computing performance. Choose from 'rmse', 'mae',\n        'pearson' and 'spearman'.\n        By default 'rmse'.\n    training_method : str, optional\n        Traning method selection. Select from 'dnn' or 'dnn_lossweights'.\n        By default 'dnn'\n    verbose : int, optional\n        Control the verbosity.\n        By default 0, which means no screen feedback.\n    \"\"\"\n    # Data normalization\n    logger.debug(f'Normalizing input/output data. Method: {normalize_method}.')\n    self.gpi_input[:], scaler_input = normalize(self.gpi_input,\n                                                normalize_method)\n    self.gpi_output[:], scaler_output = normalize(self.gpi_output,\n                                                  normalize_method)\n\n    # Data split\n    input_years = self.gpi_input.index.year\n    output_years = self.gpi_output.index.year\n    logger.debug(\n        'Spliting Trainning and validation data. Split year: {}.'.format(\n            self.val_split_year))\n    jackknife_input = self.gpi_input[input_years &lt; self.val_split_year]\n    jackknife_output = self.gpi_output[output_years &lt; self.val_split_year]\n    vali_input = self.gpi_input[input_years &gt;= self.val_split_year]\n    vali_output = self.gpi_output[output_years &gt;= self.val_split_year]\n    year_list = jackknife_input.index.year.unique()\n\n    # Jackknife in time\n    loo = LeaveOneOut()\n    best_perf_sum = None\n    for train_index, test_index in loo.split(year_list): # noqa: B007\n        this_year = year_list[test_index[0]]\n\n        input_years = jackknife_input.index.year\n        output_years = jackknife_output.index.year\n\n        logger.info(f'Jackknife on year: {this_year}.')\n        train_input = jackknife_input[input_years != this_year]\n        train_output = jackknife_output[output_years != this_year]\n\n        # check if train_input and train_output are empty, raise value error\n        if train_input.empty or train_output.empty:\n            raise ValueError(\n                'Trainning data is empty. Please check the val_split_year.'\n                )\n\n        test_input = jackknife_input[input_years == this_year]\n        test_output = jackknife_output[output_years == this_year]\n\n        # check if test_input and test_output are empty, raise value error\n        if test_input.empty or test_output.empty:\n            raise ValueError(\n                'Testing data is empty. Please check the val_split_year.'\n                )\n\n        # Execute training\n        training = NNTrain(train_input, train_output)\n\n        # Set searching space\n        training.update_space(**searching_space)\n\n        # Optimization\n        training.optimize(**optimize_space,\n                          training_method=training_method,\n                          verbose=verbose)\n\n        # TODO: Add warning if no model selected for the year\n        if training.model is None:\n            logger.warning(f'No best model was found for year: {str(this_year)}.')\n            continue\n\n        if self.export_all_years:\n            path_model = f'{self.outpath}/all_years/optimized_model_{this_year}'\n            training.export(path_model=path_model)\n\n        # find minimum rmse\n        # TODO: mae, pearson, spearman\n        apr_perf = performance(test_input, test_output, training.model,\n                               performance_method, scaler_output)\n        perf_sum = np.nansum(apr_perf)\n        if best_perf_sum is None:\n            best_perf_sum = perf_sum\n        if perf_sum &lt;= best_perf_sum:\n            self.apr_perf = apr_perf\n            self.post_perf = performance(vali_input, vali_output,\n                                         training.model,\n                                         performance_method, scaler_output)\n            self.best_train = training\n            self.best_year = this_year\n    logger.info(f'Found best year: {str(self.best_year)}'\n                f'A-priori performance: {self.apr_perf}'\n                f'Post-priori performance: {self.post_perf}')\n</code></pre>"},{"location":"api_reference/#utility-functions","title":"Utility Functions:","text":""},{"location":"api_reference/#motrainer.util","title":"<code>motrainer.util</code>","text":""},{"location":"api_reference/#motrainer.util.normalize","title":"<code>normalize(data, method)</code>","text":"<p>Pre-normalization for input/output.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrAME</code> <p>Data to normalize.</p> required <code>method</code> <code>str</code> <p>Data to normalize. Choose from 'standard' or 'min_max'.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of [data_norm, scaler]. Normalized data and scaler used for normalization.</p> Source code in <code>motrainer/util.py</code> <pre><code>def normalize(data, method):\n    \"\"\"Pre-normalization for input/output.\n\n    Parameters\n    ----------\n    data : pandas.DataFrAME\n        Data to normalize.\n    method : str\n        Data to normalize. Choose from 'standard' or 'min_max'.\n\n    Returns\n    -------\n    list\n        A list of [data_norm, scaler]. Normalized data and scaler used for\n        normalization.\n\n    \"\"\"\n    if method == 'standard':\n        scaler = sklearn.preprocessing.StandardScaler()\n    elif method == 'min_max':\n        scaler = sklearn.preprocessing.MinMaxScaler()\n    else:\n        raise NotImplementedError\n\n    data_norm = scaler.fit_transform(data)\n    return data_norm, scaler\n</code></pre>"},{"location":"api_reference/#motrainer.util.performance","title":"<code>performance(data_input, data_label, model, method, scaler_output=None)</code>","text":"<p>Compute performance of trained neuron netowrk.</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>DataFrame</code> <p>Input data.</p> required <code>data_label</code> <code>DataFrame</code> <p>Label data.</p> required <code>model</code> <code>models</code> <p>Trained model to compute performance.</p> required <code>method</code> <code>str</code> <p>Method to compute</p> required <code>scaler_output</code> <code>optional</code> <p>Scaler of output, by default None. When not None, function will assume that a normalization has been performed to output, and will use scaler_output to transform the output back to the original scale.</p> <code>None</code> <p>Returns:</p> Type Description <code>float or list of float</code> <p>Performance value. If the model gives multiple output, the performance will be a list.</p> Source code in <code>motrainer/util.py</code> <pre><code>def performance(data_input, data_label, model, method, scaler_output=None):\n    \"\"\"Compute performance of trained neuron netowrk.\n\n    Parameters\n    ----------\n    data_input : pandas.DataFrame\n        Input data.\n    data_label : pandas.DataFrame\n        Label data.\n    model : tf.keras.models\n        Trained model to compute performance.\n    method : str\n        Method to compute\n    scaler_output : optional\n        Scaler of output, by default None.\n        When not None, function will assume that a normalization has been\n        performed to output, and will use scaler_output to transform the output\n        back to the original scale.\n\n    Returns\n    -------\n    float or list of float\n        Performance value. If the model gives multiple output, the performance\n        will be a list.\n    \"\"\"\n    # Temporally SL the model because of TF graph execution issue\n    # TODO: fix the model prediction issue\n    tmp_path = f'/tmp/tmp_model{random.getrandbits(64)}'\n    model.save(tmp_path)\n    model = tf.keras.models.load_model(tmp_path)\n    predicted = model.predict(data_input)\n\n    # In case multiple outputs, re-arrange to one df\n    if isinstance(predicted, list):\n        predicted = np.hstack(predicted)\n\n    # Scale back if the data was normalized\n    if scaler_output is not None:\n        re_predicted = scaler_output.inverse_transform(predicted, 'f')\n        re_label = scaler_output.inverse_transform(data_label, 'f')\n    else:\n        re_predicted = predicted\n        re_label = data_label\n\n    difference = re_predicted - re_label\n    perf = np.zeros([predicted.shape[1], 1])\n    if method == 'rmse':\n        for j in range(predicted.shape[1]):\n            perf[j, 0] = np.round(np.sqrt(((difference[j])**2).mean()), 5)\n    elif method == 'mae':\n        for j in range(predicted.shape[1]):\n            perf[j, 0] = np.round((difference[j].mean()), 5)\n    elif method == 'pearson':\n        for j in range(predicted.shape[1]):\n            perf[j, 0] = np.round(pearsonr(re_predicted[:, j], re_label[:, j]),\n                                  5)[0]\n    elif method == 'spearman':\n        for j in range(predicted.shape[1]):\n            perf[j,\n                 0] = np.round(spearmanr(re_predicted[:, j], re_label[:, j]),\n                               5)[0]\n\n    return perf\n</code></pre>"},{"location":"api_reference/#motrainer.util.sklearn_load","title":"<code>sklearn_load(path_model)</code>","text":"<p>Load sklearn model from hdf5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path_model</code> <code>str</code> <p>Path to the model.</p> required <p>Returns:</p> Type Description <code>model</code> <p>Sklearn model.</p> Source code in <code>motrainer/util.py</code> <pre><code>def sklearn_load(path_model):\n    \"\"\"Load sklearn model from hdf5 file.\n\n    Parameters\n    ----------\n    path_model : str\n        Path to the model.\n\n    Returns\n    -------\n    sklearn.model\n        Sklearn model.\n\n    \"\"\"\n    with h5py.File(path_model, 'r') as f:\n        if 'model' not in f.attrs:\n            raise ValueError(\"No model found in the hdf5 file.\")\n\n        model_base64 = f.attrs['model']\n\n        # Decode the bytes\n        model_bytes = base64.b64decode(model_base64)\n\n        # Load the model\n        model = pickle.loads(model_bytes)\n\n        meta_data = {}\n        for key in f.attrs.keys():\n            if key != 'model':\n                meta_data[key] = f.attrs[key]\n\n\n    return model, meta_data\n</code></pre>"},{"location":"api_reference/#motrainer.util.sklearn_save","title":"<code>sklearn_save(model, path_model, meta_data=None)</code>","text":"<p>Save sklearn model to hdf5 file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>model</code> <p>Sklearn model to save.</p> required <code>path_model</code> <code>str</code> <p>Path to save the model.</p> required <code>meta_data</code> <code>Dict</code> <p>optional. A dict of meta data to save.</p> <code>None</code> Source code in <code>motrainer/util.py</code> <pre><code>def sklearn_save(model, path_model, meta_data=None):\n    \"\"\"Save sklearn model to hdf5 file.\n\n    Parameters\n    ----------\n    model : sklearn.model\n        Sklearn model to save.\n    path_model : str\n        Path to save the model.\n    meta_data : Dict, optional\n        optional. A dict of meta data to save.\n\n    \"\"\"\n    model_bytes = pickle.dumps(model)\n\n    # Encode the bytes as base64\n    model_base64 = base64.b64encode(model_bytes)\n\n    with h5py.File(path_model, 'w') as f:\n        f.attrs['model'] = model_base64\n\n        if meta_data is not None:\n            for key, value in meta_data.items():\n                f.attrs[key] = value\n</code></pre>"},{"location":"setup/","title":"Installation","text":"<p>MOTrainer can be installed from PyPI:</p> <pre><code>pip install motrainer\n</code></pre> <p>or from the source:</p> <pre><code>git clone git@github.com:VegeWaterDynamics/motrainer.git\ncd motrainer\npip install .\n</code></pre> <p>Note that Python version <code>&gt;=3.10</code> and <code>&lt;3.12</code> is required to install MOTrainer.</p>"},{"location":"setup/#tips","title":"Tips","text":"<p>We strongly recommend installing separately from your default Python envrionment. E.g. you can use enviroment manager e.g. mamba to create separate environment.</p>"},{"location":"usage_daskml/","title":"Parallel ML Model Optimization with Dask-ml","text":"<p>In the data split section, we discussed how to partition a dataset. After the partitioning process, the dataset is divided into two data bags: <code>train_bags</code> and <code>test_bags</code>.</p> <p>Following dataset partitioning, you can execute parallel machine learning training using <code>dask-ml</code>. Dask-ml leverages Dask alongside popular machine learning libraries like Scikit-Learn to facilitate distributed machine learning tasks. This document provides a brief overview of its usage. For a more comprehensive example, refer to this Jupyter Notebook.</p>"},{"location":"usage_daskml/#extract-training-data","title":"Extract training data","text":"<p>With <code>dask-ml</code>, the input and output data need be converted to a DataFrame. You can map this conversion to each partition as follows:</p> <pre><code>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n\ntrain_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe)\n</code></pre>"},{"location":"usage_daskml/#configuring-the-search-grid","title":"Configuring the Search Grid","text":"<p>You can set up distributed machine learning training jobs using <code>dask-ml.model_selection</code>. For instance, you can perform an exhaustive search over specified parameter values for an estimator using <code>GridSearchCV</code>:</p> <pre><code>import dask_ml.model_selection as dcv\nfrom sklearn import svm, datasets\n\nparameters = {'kernel': ['linear', 'rbf'], 'C': [1, 10]}\nsvc = svm.SVC()\nsearch = dcv.GridSearchCV(svc, parameters)\n</code></pre>"},{"location":"usage_daskml/#executing-grid-search-in-parallel","title":"Executing Grid Search in Parallel","text":"<p>After setting up the search object, you can map the <code>fit</code> function to each partition as follows:</p> <pre><code>def optimize(df, grid_search, input_list, output_list):\n    \"\"\"Customized Optimization Function\n    \"\"\"\n    grid_result = grid_search.fit(df[input_list], df[output_list])\n    return grid_result\n\ninput_list = [\"STATE1\", \"STATE2\", \"STATE3\", \"STATE4\", \"STATE5\"]\noutput_list = [\"slop\"]\n\n# When memory allows, peresist data in the workers\n# When train_bags can is lazilly loading from file system, this can avoid redundant data loading\ntrain_bags = train_bags.persist()\n\noptimazed_estimators = train_bags.map(\n    optimize, search, input_list, output_list\n)\n</code></pre> <p>In this way, the machine learning training task is parallelized on two levels:</p> <ol> <li>The training jobs of all partitions of the dataset are parallelized;</li> <li>The parameter searching within each partition is parallelized.</li> </ol> <p>By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the Dask Documentation.</p>"},{"location":"usage_dnn/","title":"Parallel DNN Training with JackknifeGPI","text":"<p>The data split section explains how to partition a dataset. After partitioning, the dataset is divided into two data bags: <code>train_bags</code> and <code>test_bags</code>.</p> <p>This section demonstrates how to perform distributed DNN training using Tensorflow, with Jackknife resampling as the cross-validation method. In this approach, each year is iteratively left out as the cross-validation data. The best model is selected based on the lowest Root Mean Square Error (RMSE).</p> <p>A more comprehensive example can be found in this Example Notebook.</p>"},{"location":"usage_dnn/#training-a-single-grid-point","title":"Training a Single Grid Point","text":"<p>To train a DNN for a single grid cell, you can use the <code>JackknifeGPI</code> object:</p> <pre><code>from motrainer.jackknife import JackknifeGPI\n\n# Intiate a Jackknife GPI from one gridcell\ndf = train_bags.take(1)\ngpi_data = df.compute()\ngpi = JackknifeGPI(gpi_data, outpath='./results')\n\n# Perform training and export\nresults = gpi.train()\ngpi.export_best()\n</code></pre> <p>The training results will be exported to the <code>./results</code>path.</p>"},{"location":"usage_dnn/#training-multiple-grid-points","title":"Training Multiple Grid Points","text":"<p>To train multiple grid points in parallel, you can define a training function as follows:</p> <pre><code>def training_func(gpi_num, df):\n    gpi_data = df.compute()\n\n    gpi = JackknifeGPI(gpi_data,\n                       outpath=f\"results/gpi{gpi_num}\")\n\n    gpi.train()\n    gpi.export_best()\n</code></pre> <p>Then, map the training function to each grid cell:</p> <pre><code>from dask.distributed import Client, wait\n\n# Use client to parallelize the loop across workers\nclient = Client()\nfutures = [\n    client.submit(training_func, gpi_num, df) for gpi_num, df in enumerate(train_bags)\n]\n\n# Wait for all computations to finish\nwait(futures)\n\n# Get the results\nresults = client.gather(futures)\n</code></pre> <p>The above examples uses a local threaded Dask scheduler to parallelize the tasks. When executing training on an HPC system, we recommend using Dask SLURM cluster for the distributed training. For more information on different Dask clusters, please check the Dask Documentation.</p> <p>You can also directly submit training jobs as SLURM jobs, instead of using Dask SLURM cluster. You can find the example of using SLURM here.</p>"},{"location":"usage_split/","title":"Data Split","text":"<p>This section demonstrates how to use motrainer to partition data for parallel ML model training.</p> <p>We start with an xarray.Dataset object, which contains spatio-temporal data for training multiple independent Machine Learning (ML) models. Examples of reading spatio-temporal data into <code>xarray.Dataset</code> can be found in Example 1 and Example 2.</p> <p>The example Dataset object <code>ds</code> contains input and output data from 5 grid cells. It has two dimensions: <code>space</code> and <code>time</code>. It has six data variables, where <code>STATE1</code> to <code>STATE5</code> are physical model states (input data) and observations (output data).</p> <pre><code>print(ds)\n</code></pre> <pre><code>&lt;xarray.Dataset&gt;\nDimensions:    (space: 5, time: 8506)\nCoordinates:\n    latitude   (space) float64 56.12 46.12 53.38 49.38 44.38\n    longitude  (space) float64 11.38 6.625 6.125 12.38 0.625\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\nDimensions without coordinates: space\nData variables:\n    STATE1     (space, time) float64 0.07079 0.05532 0.04846 ... 0.06712 0.05521\n    STATE2     (space, time) float64 0.04366 0.0462 0.03821 ... 0.0861 0.05622\n    STATE3     (space, time) float64 280.0 270.4 285.5 277.4 ... 287.4 272.1\n    STATE4     (space, time) float64 274.8 278.4 280.6 283.7 ... 280.2 281.5\n    STATE5     (space, time) float64 280.9 279.7 278.0 278.0 ... 281.2 280.1\n    OBS        (space, time) float64 -9.49 -8.494 -9.069 ... -8.071 -8.237\nAttributes:\n    license:  data license\n    source:   data source\n</code></pre> <p>Before splitting, you can verify if a dataset is splittable using the <code>is_splitable</code> function:</p> <pre><code>motrainer.is_splitable(ds)\n</code></pre> <pre><code>True\n</code></pre> <p>The is_splitable function will return True if the dataset has exactly two dimensions: \"space\" and \"time\", and there are no duplicated keys in any of the coordinates of the dataset.</p>"},{"location":"usage_split/#splitting-spatio-temporal-dataset-for-independent-training-processes","title":"Splitting Spatio-Temporal Dataset for Independent Training Processes","text":"<p>The dataset_split function can be used to partition data. The split can be performed by specifying a dimension name or using an identifier.</p>"},{"location":"usage_split/#splitting-by-dimension-names","title":"Splitting by Dimension Names","text":"<p>One can specify adimension name to split, e.g. \"space\":</p> <p><pre><code>import motrainer\nbags = motrainer.dataset_split(ds, \"space\")\nprint(bags)\n</code></pre> <pre><code>dask.bag&lt;from_sequence, npartitions=5&gt;\n</code></pre></p> <p>This will split <code>ds</code> per grid cell in to a <code>Dask.bag</code> object. Each partition is an independent gridcell. </p> <p>We can check one grid cell by:</p> <p><pre><code>print(bags.take(1))\n</code></pre> <pre><code>(&lt;xarray.Dataset&gt;\n Dimensions:    (space: 1, time: 8506)\n Coordinates:\n     latitude   (space) float64 56.12\n     longitude  (space) float64 11.38\n   * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\n     space_id   (space) int64 0\n Dimensions without coordinates: space\n Data variables:\n     STATE1     (space, time) float64 0.07079 0.05532 0.04846 ... 0.06611 0.06511\n     STATE2     (space, time) float64 0.04366 0.0462 0.03821 ... 0.0361 0.05612\n     STATE3     (space, time) float64 280.0 270.4 285.5 277.4 ... 282.4 278.7\n     STATE4     (space, time) float64 274.8 278.4 280.6 283.7 ... 281.2 281.9\n     STATE5     (space, time) float64 280.9 279.7 278.0 278.0 ... 276.1 279.1\n     OBS        (space, time) float64 -9.49 -8.494 -9.069 ... -8.1721 -8.157\n Attributes:\n     license:  data license\n     source:   data source,)\n</code></pre></p>"},{"location":"usage_split/#1-d-split-by-indetifier","title":"1-D split by indetifier","text":"<p>One can also create an identifier dictionary to split. The keys of the dictionary should be a subset of {\"space\", \"time\"}, mapping \"space\" and/or \"time\" dimension with corresponding separation identifier.</p> <p>For example, <code>ds</code> can be splitted into two parts (first+fourth grid, and the rest) in space:</p> <pre><code>import numpy as np\n\nidentifier = {\"space\": np.array([0, 1, 1, 0, 1])}\nbags = motrainer.dataset_split(ds, identifier)\n\nds_splitted = bags.compute()\nprint(ds_splitted[0].dims['space'])\nprint(ds_splitted[1].dims['space'])\n</code></pre> <pre><code>2\n3\n</code></pre>"},{"location":"usage_split/#2-d-split-by-indetifier","title":"2-D split by indetifier","text":"<p>One can perform a 2-D split by providing identifiers in both space and time dimensions:</p> <p><pre><code>id_time = np.zeros(8506)\nid_time[2000:]=1\nidentifier = {\"space\": np.array([0, 1, 1, 0, 1]), \"time\": id_time}\nbags = motrainer.dataset_split(ds, identifier)\n\nprint(bags)\n</code></pre> <pre><code>dask.bag&lt;from_sequence, npartitions=4&gt;\n</code></pre></p> <p><pre><code>for parts in ds_splitted:\n    print(parts.dims)\n</code></pre> <pre><code>Frozen({'samples': 4000})\nFrozen({'samples': 13012})\nFrozen({'samples': 6000})\nFrozen({'samples': 19518})\n</code></pre></p>"},{"location":"usage_split/#train-test-split","title":"Train-Test Split","text":"<p>Before training, one can further split the datasets into training and testing datasets to reserve some data for testing. If the split needs to be based on the space and time coordinates, one can use <code>motrainer.train_test_split</code> to perform this split. Otherwise we recommend <code>sklearn.model_selection.train_test_split</code>(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for simplicity.</p>"},{"location":"usage_split/#splitting-by-coordinates","title":"Splitting by Coordinates","text":"<p>When specifying a space or time coordinate, the dataset will be split into two parts: the part smaller (\"&lt;\") than the coordinates and the rest (\"&gt;=\"). By default, the former will be the training data and the latter will be testing.</p> <p>If the dataset has already be splitted into <code>Dask.bags</code>, we recommend to use the <code>map</code> function to apply train-test split to each splitted element.</p> <p>The following will select data before 2017-01-01 as training data:</p> <pre><code>train_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2017-01-01\")}\n)\n</code></pre> <p>Then extract train and test data using <code>pluck</code>:</p> <pre><code>train_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</code></pre> <p>When <code>reverse=True</code> is present, the latter part (\"&gt;=\" coordinate) will be training data, the rest will be testing. The following code will select data after (and include) 2017-01-01 as training data:</p> <pre><code>train_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2017-01-01\"), reverse=True}\n)\ntrain_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</code></pre> <p>One can also apply <code>motrainer.train_test_split</code> directly to an <code>xarray.Dataset</code> object: <pre><code>motrainer.train_test_split(ds, split={\"time\": np.datetime64(\"2017-01-01\")})\n</code></pre></p>"},{"location":"usage_split/#splitting-by-mask","title":"Splitting by Mask","text":"<p>Alternatively, you can also initiate a <code>mask</code> to perform training data. By default, training data will be where <code>mask</code> is <code>True</code>. For example, if you would like to have data before 2017-01-01 as training data:</p> <pre><code>mask = ds_valid[\"time\"] &lt; np.datetime64(\"2017-01-01\")\ntrain, test = train_test_split(ds_valid, mask=mask)\n</code></pre> <p>If <code>reverse</code> is specified, training data will be where mask is <code>False</code>. The following will select data after (and include) 2017-01-01 as training data:</p> <pre><code>mask = ds_valid[\"time\"] &lt; np.datetime64(\"2017-01-01\")\ntrain, test = train_test_split(ds_valid, mask=mask, reverse=True)\n</code></pre>"},{"location":"notebooks/example_daskml/","title":"Prallely training sklearn models with dask-ml","text":"<p>This notebooks demonstrate how to execute parallel machine learning training using <code>dask-ml</code> and motrainer.</p> <p>The example dataset <code>./example1_data.zarr/</code> can be generated using this Jupyter Notebook.</p> In\u00a0[1]: Copied! <pre>import motrainer\nimport numpy as np\nimport xarray as xr\n</pre> import motrainer import numpy as np import xarray as xr In\u00a0[2]: Copied! <pre>data_path = \"./example1_data.zarr\"\nds = xr.open_zarr(data_path)\nds\n</pre> data_path = \"./example1_data.zarr\" ds = xr.open_zarr(data_path) ds Out[2]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (space: 5, time: 8506)\nCoordinates:\n    latitude   (space) float64 dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;\n    longitude  (space) float64 dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\nDimensions without coordinates: space\nData variables:\n    BIOMA1     (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    BIOMA2     (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG1        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG2        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    TG3        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG1        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG2        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    WG3        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    curv       (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    sig        (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\n    slop       (space, time) float64 dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;\nAttributes:\n    license:  data license\n    source:   data source</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>space: 5</li><li>time: 8506</li></ul></li><li>Coordinates: (3)<ul><li>latitude(space)float64dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;  Array   Chunk   Bytes   40 B   40 B   Shape   (5,)   (5,)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  5 1 </li><li>longitude(space)float64dask.array&lt;chunksize=(5,), meta=np.ndarray&gt;  Array   Chunk   Bytes   40 B   40 B   Shape   (5,)   (5,)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  5 1 </li><li>time(time)datetime64[ns]2007-01-02 ... 2020-01-01T01:00:00<pre>array(['2007-01-02T00:00:00.000000000', '2007-01-03T00:00:00.000000000',\n       '2007-01-03T00:30:00.000000000', ..., '2020-01-01T00:00:00.000000000',\n       '2020-01-01T00:30:00.000000000', '2020-01-01T01:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (11)<ul><li>BIOMA1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>BIOMA2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>TG3(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG1(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG2(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>WG3(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>curv(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>sig(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li><li>slop(space, time)float64dask.array&lt;chunksize=(3, 8506), meta=np.ndarray&gt;  Array   Chunk   Bytes   332.27 kiB   199.36 kiB   Shape   (5, 8506)   (3, 8506)   Dask graph   2 chunks in 2 graph layers   Data type   float64 numpy.ndarray  8506 5 </li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2007-01-02 00:00:00', '2007-01-03 00:00:00',\n               '2007-01-03 00:30:00', '2007-01-05 00:00:00',\n               '2007-01-06 00:00:00', '2007-01-08 00:00:00',\n               '2007-01-08 00:30:00', '2007-01-09 00:00:00',\n               '2007-01-10 00:00:00', '2007-01-11 00:00:00',\n               ...\n               '2019-12-29 01:00:00', '2019-12-30 00:00:00',\n               '2019-12-30 00:30:00', '2019-12-30 01:00:00',\n               '2019-12-30 01:30:00', '2019-12-31 00:00:00',\n               '2019-12-31 00:30:00', '2020-01-01 00:00:00',\n               '2020-01-01 00:30:00', '2020-01-01 01:00:00'],\n              dtype='datetime64[ns]', name='time', length=8506, freq=None))</pre></li></ul></li><li>Attributes: (2)license :data licensesource :data source</li></ul> In\u00a0[3]: Copied! <pre># Check if the dataset is splitable\nmotrainer.is_splitable(ds)\n</pre> # Check if the dataset is splitable motrainer.is_splitable(ds) Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># split the dataset per grid cell\nbags = motrainer.dataset_split(ds, \"space\")\nbags\n</pre> # split the dataset per grid cell bags = motrainer.dataset_split(ds, \"space\") bags Out[4]: <pre>dask.bag&lt;from_sequence, npartitions=5&gt;</pre> In\u00a0[5]: Copied! <pre>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n</pre> def to_dataframe(ds):     return ds.to_dask_dataframe()  def chunk(ds, chunks):     return ds.chunk(chunks) In\u00a0[6]: Copied! <pre># Train test split, mapped to each element of the bag\ntrain_test_bags = bags.map(\n    motrainer.train_test_split, split={\"time\": np.datetime64(\"2016-01-01\")}\n)\n\n# # Or split by mask\n# mask = ds[\"time\"]&lt;np.datetime64(\"2016-01-01\")\n# train_test_bags = bags.map(\n#     motrainer.train_test_split, mask={\"time\": np.datetime64(\"2016-01-01\")}\n# )\n</pre> # Train test split, mapped to each element of the bag train_test_bags = bags.map(     motrainer.train_test_split, split={\"time\": np.datetime64(\"2016-01-01\")} )  # # Or split by mask # mask = ds[\"time\"] In\u00a0[7]: Copied! <pre># Retrieve the train and test bags\ntrain_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe)\ntest_bags = train_test_bags.pluck(1).map(chunk, {\"space\": 500}).map(to_dataframe)\n</pre> # Retrieve the train and test bags train_bags = train_test_bags.pluck(0).map(chunk, {\"space\": 500}).map(to_dataframe) test_bags = train_test_bags.pluck(1).map(chunk, {\"space\": 500}).map(to_dataframe) In\u00a0[8]: Copied! <pre># Setup grid search\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RepeatedKFold\n\nfrom dask_ml.preprocessing import MinMaxScaler\nfrom dask_ml.model_selection import GridSearchCV\n\nregSVR = make_pipeline(MinMaxScaler(), SVR())\nkernel = [\"poly\", \"rbf\", \"sigmoid\"]\nC = [1, 0.1]\ngamma = [\"scale\"]\ngrid = dict(svr__kernel=kernel, svr__C=C, svr__gamma=gamma)\ncv = RepeatedKFold(n_splits=4, n_repeats=2, random_state=1)\ngrid_search = GridSearchCV(\n    estimator=regSVR,\n    param_grid=grid,\n    cv=cv,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    refit=\"r2\",\n)\n</pre> # Setup grid search from sklearn.svm import SVR from sklearn.pipeline import make_pipeline from sklearn.model_selection import RepeatedKFold  from dask_ml.preprocessing import MinMaxScaler from dask_ml.model_selection import GridSearchCV  regSVR = make_pipeline(MinMaxScaler(), SVR()) kernel = [\"poly\", \"rbf\", \"sigmoid\"] C = [1, 0.1] gamma = [\"scale\"] grid = dict(svr__kernel=kernel, svr__C=C, svr__gamma=gamma) cv = RepeatedKFold(n_splits=4, n_repeats=2, random_state=1) grid_search = GridSearchCV(     estimator=regSVR,     param_grid=grid,     cv=cv,     scoring=[\"r2\", \"neg_mean_squared_error\"],     refit=\"r2\", ) In\u00a0[9]: Copied! <pre># Setup optimization function\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\n\ndef optimize(df, grid_search, input_list, output_list):\n    \"\"\"Customized Optimization Function\n    \"\"\"\n    df = df.dropna()\n\n    # Because a dask dataframe is a delayed object, fit function raises warning DataConversionWarning. \n    # Here the warning is supressed. \n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n        grid_result = grid_search.fit(df[input_list], df[output_list])\n    return grid_result\n\n# Map the optimization function to the train bags\ninput_list = [\"BIOMA1\", \"BIOMA1\", \"TG1\", \"TG2\", \"TG3\"]\noutput_list = [\"slop\"]\noptimazed_estimators = train_bags.map(\n    optimize, grid_search=grid_search, input_list=input_list, output_list=output_list\n)\n</pre> # Setup optimization function import warnings from sklearn.exceptions import DataConversionWarning  def optimize(df, grid_search, input_list, output_list):     \"\"\"Customized Optimization Function     \"\"\"     df = df.dropna()      # Because a dask dataframe is a delayed object, fit function raises warning DataConversionWarning.      # Here the warning is supressed.      with warnings.catch_warnings():         warnings.filterwarnings(\"ignore\", category=DataConversionWarning)         grid_result = grid_search.fit(df[input_list], df[output_list])     return grid_result  # Map the optimization function to the train bags input_list = [\"BIOMA1\", \"BIOMA1\", \"TG1\", \"TG2\", \"TG3\"] output_list = [\"slop\"] optimazed_estimators = train_bags.map(     optimize, grid_search=grid_search, input_list=input_list, output_list=output_list ) In\u00a0[10]: Copied! <pre># Execute the training\noptimazed_estimators_realized = optimazed_estimators.compute()\n</pre> # Execute the training optimazed_estimators_realized = optimazed_estimators.compute() In\u00a0[11]: Copied! <pre>from motrainer import util\n</pre> from motrainer import util In\u00a0[12]: Copied! <pre>for id, model in enumerate(optimazed_estimators_realized):\n    util.sklearn_save(model, f\"./results/{id}_model.h5\", meta_data = {\"data_source\": data_path})\n    print(f\"{id}_model.h5 is saved\")\n</pre> for id, model in enumerate(optimazed_estimators_realized):     util.sklearn_save(model, f\"./results/{id}_model.h5\", meta_data = {\"data_source\": data_path})     print(f\"{id}_model.h5 is saved\") <pre>0_model.h5 is saved\n1_model.h5 is saved\n2_model.h5 is saved\n3_model.h5 is saved\n4_model.h5 is saved\n</pre> In\u00a0[14]: Copied! <pre># Load the models back\nfrom pathlib import Path\nlist_model = []\n\nfor file in Path(\"./results\").glob(\"*.h5\"):\n    model, metadata = util.sklearn_load(file)\n    list_model.append(model)\nlist_model\n</pre> # Load the models back from pathlib import Path list_model = []  for file in Path(\"./results\").glob(\"*.h5\"):     model, metadata = util.sklearn_load(file)     list_model.append(model) list_model Out[14]: <pre>[GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error']),\n GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=4, random_state=1),\n              estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n                                        ('svr', SVR())]),\n              param_grid={'svr__C': [1, 0.1], 'svr__gamma': ['scale'],\n                          'svr__kernel': ['poly', 'rbf', 'sigmoid']},\n              refit='r2', scoring=['r2', 'neg_mean_squared_error'])]</pre> In\u00a0[15]: Copied! <pre>from sklearn.metrics import mean_squared_error , r2_score,  mean_absolute_error\n\n# This for need to be coverted to a user defined \nlist_metrics = []\nfor model, test_data in zip(list_model, test_bags.compute()):\n    test_data = test_data.dropna()\n    X_test = test_data[input_list]\n    Y_test = test_data[output_list]\n    Y_eval = model.predict(X_test)\n\n    metrics = {\"MSE_SVR\": mean_squared_error(Y_test,Y_eval),\n               \"MAE_SVR\": mean_absolute_error(Y_test,Y_eval),\n               \"R_2\":r2_score(Y_test,Y_eval)}\n    list_metrics.append(metrics)\n</pre> from sklearn.metrics import mean_squared_error , r2_score,  mean_absolute_error  # This for need to be coverted to a user defined  list_metrics = [] for model, test_data in zip(list_model, test_bags.compute()):     test_data = test_data.dropna()     X_test = test_data[input_list]     Y_test = test_data[output_list]     Y_eval = model.predict(X_test)      metrics = {\"MSE_SVR\": mean_squared_error(Y_test,Y_eval),                \"MAE_SVR\": mean_absolute_error(Y_test,Y_eval),                \"R_2\":r2_score(Y_test,Y_eval)}     list_metrics.append(metrics) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/example_daskml/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/example_daskml/#split-per-gridcell","title":"Split per gridcell\u00b6","text":""},{"location":"notebooks/example_daskml/#train-test-split","title":"Train Test Split\u00b6","text":""},{"location":"notebooks/example_daskml/#setup-training","title":"Setup Training\u00b6","text":""},{"location":"notebooks/example_daskml/#model-optimization","title":"Model Optimization\u00b6","text":""},{"location":"notebooks/example_daskml/#save-model","title":"Save model\u00b6","text":""},{"location":"notebooks/example_daskml/#model-performance-evaluation","title":"Model performance evaluation\u00b6","text":""},{"location":"notebooks/example_dnn/","title":"Prallely training DNN with Tensorflow","text":"<p>This notebooks demonstrate how to split data to train-test execute parallel DNN trainings.</p> <p>The example dataset <code>./example1_data.zarr/</code> can be generated using this Jupyter Notebook.</p> In\u00a0[15]: Copied! <pre>import xarray as xr\nimport motrainer\nimport dask_ml.model_selection as dcv\nfrom motrainer.jackknife import JackknifeGPI\n</pre> import xarray as xr import motrainer import dask_ml.model_selection as dcv from motrainer.jackknife import JackknifeGPI In\u00a0[16]: Copied! <pre># Read the data\nzarr_file_path = \"./example1_data.zarr\"\nds = xr.open_zarr(zarr_file_path)\n</pre> # Read the data zarr_file_path = \"./example1_data.zarr\" ds = xr.open_zarr(zarr_file_path) In\u00a0[17]: Copied! <pre>def to_dataframe(ds):\n    return ds.to_dask_dataframe()\n\ndef chunk(ds, chunks):\n    return ds.chunk(chunks)\n    \nbags = motrainer.dataset_split(ds, \"space\")\nbags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)\n\ntest_size = 0.33\nf_shuffle = True\ntrain_test_bags = bags.map(\n    dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1\n)  \ntrain_bags = train_test_bags.pluck(0)\ntest_bags = train_test_bags.pluck(1)\n</pre> def to_dataframe(ds):     return ds.to_dask_dataframe()  def chunk(ds, chunks):     return ds.chunk(chunks)      bags = motrainer.dataset_split(ds, \"space\") bags = bags.map(chunk, {\"space\": 100}).map(to_dataframe)  test_size = 0.33 f_shuffle = True train_test_bags = bags.map(     dcv.train_test_split, test_size=test_size, shuffle=f_shuffle, random_state=1 )   train_bags = train_test_bags.pluck(0) test_bags = train_test_bags.pluck(1) In\u00a0[18]: Copied! <pre># JackKnife parameters\nJackKnife = {\n    'val_split_year': 2017,\n    'output_list': ['sig', 'slop', 'curv'],\n    'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],\n    'out_path': './dnn_examples/results'\n}\n\n# Training parameters\nsearching_space = {\n    'num_dense_layers': [1, 10],\n    'num_input_nodes': [1, 6],\n    'num_dense_nodes': [1, 128],\n    'learning_rate': [5e-4, 1e-2],\n    'activation': ['relu']\n}\n\n# Here, I reduce parameters to be able to run on my own machine\noptimize_space = {\n    'best_loss': 2, # 1\n    'n_calls': 11, # 15\n    'epochs': 5, # 300\n    'noise': 0.1, \n    'kappa': 5,\n    'validation_split': 0.2,\n    'x0': [1e-3, 1, 4, 13, 'relu', 64]\n} # For weightling loss: 'loss_weights': [1, 1, 0.5],\n</pre> # JackKnife parameters JackKnife = {     'val_split_year': 2017,     'output_list': ['sig', 'slop', 'curv'],     'input_list': ['TG1', 'TG2', 'TG3', 'WG1', 'WG2', 'WG3', 'BIOMA1', 'BIOMA2'],     'out_path': './dnn_examples/results' }  # Training parameters searching_space = {     'num_dense_layers': [1, 10],     'num_input_nodes': [1, 6],     'num_dense_nodes': [1, 128],     'learning_rate': [5e-4, 1e-2],     'activation': ['relu'] }  # Here, I reduce parameters to be able to run on my own machine optimize_space = {     'best_loss': 2, # 1     'n_calls': 11, # 15     'epochs': 5, # 300     'noise': 0.1,      'kappa': 5,     'validation_split': 0.2,     'x0': [1e-3, 1, 4, 13, 'relu', 64] } # For weightling loss: 'loss_weights': [1, 1, 0.5],  In\u00a0[19]: Copied! <pre># a function for training\ndef training_func(gpi_num, df, JackKnife, searching_space, optimize_space):\n    \n    # remove NA data\n    gpi_data = df.compute()\n    gpi_data.dropna(inplace=True)\n\n    # add time to index\n    gpi_data.set_index(\"time\", inplace=True, drop=True)\n\n    gpi = JackknifeGPI(gpi_data,\n                       JackKnife['val_split_year'],\n                       JackKnife['input_list'],\n                       JackKnife['output_list'],\n                       outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")\n\n    gpi.train(searching_space=searching_space,\n              optimize_space=optimize_space,\n              normalize_method='standard',\n              training_method='dnn',\n              performance_method='rmse',\n              verbose=2)\n\n    gpi.export_best()\n\n    return gpi.apr_perf, gpi.post_perf\n</pre> # a function for training def training_func(gpi_num, df, JackKnife, searching_space, optimize_space):          # remove NA data     gpi_data = df.compute()     gpi_data.dropna(inplace=True)      # add time to index     gpi_data.set_index(\"time\", inplace=True, drop=True)      gpi = JackknifeGPI(gpi_data,                        JackKnife['val_split_year'],                        JackKnife['input_list'],                        JackKnife['output_list'],                        outpath=f\"{JackKnife['out_path']}/gpi{gpi_num+1}\")      gpi.train(searching_space=searching_space,               optimize_space=optimize_space,               normalize_method='standard',               training_method='dnn',               performance_method='rmse',               verbose=2)      gpi.export_best()      return gpi.apr_perf, gpi.post_perf <p>By default, Dask uses a local threaded scheduler to parallelize the tasks. Alternatively, other types of clusters can be set up if the training job is running on other infrastructures. The usage of different clusters will not influence the syntax of data split and training jobs. For more information on different Dask clusters, please check the Dask Documentation.</p> In\u00a0[20]: Copied! <pre>from dask.distributed import Client\n\nclient = Client()\n</pre> from dask.distributed import Client  client = Client() In\u00a0[21]: Copied! <pre>from dask.distributed import wait\n</pre> from dask.distributed import wait In\u00a0[23]: Copied! <pre># Use client to parallelize the loop across workers\nfutures = [\n    client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags)\n]\n\n# Wait for all computations to finish\nwait(futures)\n\n# Get the results\nresults = client.gather(futures)\n</pre> # Use client to parallelize the loop across workers futures = [     client.submit(training_func, gpi_num, df, JackKnife, searching_space, optimize_space) for  gpi_num, df in enumerate(train_bags) ]  # Wait for all computations to finish wait(futures)  # Get the results results = client.gather(futures) In\u00a0[32]: Copied! <pre># Close the Dask client\nclient.close()\n</pre> # Close the Dask client client.close() In\u00a0[31]: Copied! <pre># print the results\nfor gpi_num, performance in enumerate(results):\n    print(f\"GPI {(gpi_num + 1)}\")\n    print(\" aprior performance(RMSE):\")\n    print(performance[0])\n    print(\"post performance(RMSE):\")\n    print(performance[1])\n    print(\"=========================================\")\n</pre> # print the results for gpi_num, performance in enumerate(results):     print(f\"GPI {(gpi_num + 1)}\")     print(\" aprior performance(RMSE):\")     print(performance[0])     print(\"post performance(RMSE):\")     print(performance[1])     print(\"=========================================\") <pre>GPI 1\n aprior performance(RMSE):\n[[0.26438]\n [0.03295]\n [0.14362]]\npost performance(RMSE):\n[[0.34483]\n [0.00631]\n [0.00686]]\n=========================================\nGPI 2\n aprior performance(RMSE):\n[[0.37801]\n [0.02598]\n [0.26245]]\npost performance(RMSE):\n[[0.70249]\n [0.22075]\n [0.24423]]\n=========================================\nGPI 3\n aprior performance(RMSE):\n[[0.31875]\n [0.24323]\n [0.05353]]\npost performance(RMSE):\n[[0.03498]\n [0.19958]\n [0.24324]]\n=========================================\nGPI 4\n aprior performance(RMSE):\n[[0.19431]\n [0.10026]\n [0.16398]]\npost performance(RMSE):\n[[0.20526]\n [0.02813]\n [0.21003]]\n=========================================\nGPI 5\n aprior performance(RMSE):\n[[0.23724]\n [0.1104 ]\n [0.28052]]\npost performance(RMSE):\n[[0.10751]\n [0.08874]\n [0.26091]]\n=========================================\n</pre> <p>Shutdown the client to free up the resources click on SHUTDOWN in the Dask JupyterLab extension.</p> In\u00a0[13]: Copied! <pre>import h5py\nimport tensorflow as tf\n</pre> import h5py import tensorflow as tf In\u00a0[14]: Copied! <pre>best_model = \"./dnn_examples/results/gpi1/best_optimized_model_2015.h5\"\nmodel = tf.keras.models.load_model(best_model)\nmodel.summary()\n</pre> best_model = \"./dnn_examples/results/gpi1/best_optimized_model_2015.h5\" model = tf.keras.models.load_model(best_model) model.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 5)                 45        \n                                                                 \n layer_dense_1 (Dense)       (None, 39)                234       \n                                                                 \n layer_dense_2 (Dense)       (None, 39)                1560      \n                                                                 \n layer_dense_3 (Dense)       (None, 39)                1560      \n                                                                 \n layer_dense_4 (Dense)       (None, 39)                1560      \n                                                                 \n layer_dense_5 (Dense)       (None, 39)                1560      \n                                                                 \n dense_1 (Dense)             (None, 3)                 120       \n                                                                 \n=================================================================\nTotal params: 6,639\nTrainable params: 6,639\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[16]: Copied! <pre># Add more info to the model file e.g. the path to the data\nwith h5py.File(best_model, 'a') as f:\n    f.attrs['input_file_path'] = \"./example1_data.zarr\"\n</pre> # Add more info to the model file e.g. the path to the data with h5py.File(best_model, 'a') as f:     f.attrs['input_file_path'] = \"./example1_data.zarr\" In\u00a0[17]: Copied! <pre># Inspect the hyperparameters and input_list \nwith h5py.File(best_model, 'r') as f:\n    hyperparameters = f.attrs['hyperparameters']\n    input_list = f.attrs['input_list']\n    input_file_path = f.attrs['input_file_path']\n\nprint(eval(hyperparameters))\n</pre> # Inspect the hyperparameters and input_list  with h5py.File(best_model, 'r') as f:     hyperparameters = f.attrs['hyperparameters']     input_list = f.attrs['input_list']     input_file_path = f.attrs['input_file_path']  print(eval(hyperparameters)) <pre>[(2.6945188437821344e-05, [0.0102933544822095, 2, 6, 6, 'relu', 220]), (0.0009668049169704318, [0.014053549021147586, 1, 5, 5, 'relu', 213]), (0.0011984826996922493, [0.01, 2, 5, 5, 'relu', 64]), (0.002951781963929534, [0.01034267518524073, 1, 6, 5, 'relu', 325]), (0.005780111066997051, [0.01, 1, 5, 6, 'relu', 189]), (0.0062195612117648125, [0.01238696509850767, 1, 5, 6, 'relu', 89]), (0.007209620904177427, [0.012213029952058047, 1, 6, 5, 'relu', 41]), (0.008430225774645805, [0.016252143020123296, 1, 5, 6, 'relu', 153]), (0.01519404910504818, [0.012097401107848887, 1, 5, 6, 'relu', 161]), (0.03946790099143982, [0.013412574345977837, 2, 5, 5, 'relu', 242]), (0.04046712443232536, [0.010949417026328035, 2, 5, 5, 'relu', 276]), (0.09059736132621765, [0.013515310361619494, 1, 6, 6, 'relu', 147]), (0.11357539147138596, [0.013742318889904532, 2, 5, 6, 'relu', 46]), (0.11388207972049713, [0.02, 1, 5, 6, 'relu', 123]), (0.7128570675849915, [0.012739986706827862, 1, 6, 6, 'relu', 302])]\n</pre> In\u00a0[18]: Copied! <pre>print(input_list)\n</pre> print(input_list) <pre>['TG1' 'TG2' 'TG3' 'WG1' 'WG2' 'WG3' 'BIOMA1' 'BIOMA2']\n</pre> In\u00a0[19]: Copied! <pre>print(input_file_path)\n</pre> print(input_file_path) <pre>./example1_data.zarr\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/example_dnn/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"notebooks/example_dnn/#read-data-and-split-to-train-and-test-datasets","title":"Read data and split to train and test datasets\u00b6","text":""},{"location":"notebooks/example_dnn/#define-training-parameters","title":"Define training parameters\u00b6","text":""},{"location":"notebooks/example_dnn/#run-the-training","title":"Run the training\u00b6","text":"<p>In this example, we will demonstrate how to run the training parralel per grid (partition) with a dask cluster.</p>"},{"location":"notebooks/example_dnn/#inspect-best-model-file","title":"Inspect best model file\u00b6","text":""},{"location":"notebooks/example_read_from_multiple_df/","title":"Align multiple DataFrames to a Dataset","text":"<p>An example notebook that reads data and convert it to xarray Dataset with aligned time and space axis.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt # only for plots\nfrom pathlib import Path\n</pre> import pandas as pd import xarray as xr import matplotlib.pyplot as plt # only for plots from pathlib import Path In\u00a0[2]: Copied! <pre>data_dir = \"./example_data/dssat_s1/\"\nnc_file_path = \"./example2_data.nc\"\nzarr_file_path = \"./example2_data.zarr\"\n</pre> data_dir = \"./example_data/dssat_s1/\" nc_file_path = \"./example2_data.nc\" zarr_file_path = \"./example2_data.zarr\" In\u00a0[3]: Copied! <pre>pickle_files_path = list(Path(data_dir).rglob('*.pkl'))\npickle_files_path\n</pre> pickle_files_path = list(Path(data_dir).rglob('*.pkl')) pickle_files_path Out[3]: <pre>[PosixPath('example_data/dssat_s1/DSSAT/brabant_LAI.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_CWAD.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_SWTD.pkl'),\n PosixPath('example_data/dssat_s1/DSSAT/brabant_SWTD6.pkl'),\n PosixPath('example_data/dssat_s1/Sentinel-1/Amp_CR_New.pkl')]</pre> In\u00a0[4]: Copied! <pre>for file in pickle_files_path:\n    df = pd.read_pickle(file)  # read file as pandas dataframe\n    print(file.stem)  # check file name\n    print(df.index.dtype)  # check index type\n    print(len(df.index))  # check length of index\n    print(len(df.columns))  # check length of columns\n    print(\"********\")\n</pre> for file in pickle_files_path:     df = pd.read_pickle(file)  # read file as pandas dataframe     print(file.stem)  # check file name     print(df.index.dtype)  # check index type     print(len(df.index))  # check length of index     print(len(df.columns))  # check length of columns     print(\"********\") <pre>brabant_LAI\ndatetime64[ns]\n148\n1283\n********\nbrabant_CWAD\ndatetime64[ns]\n148\n1283\n********\nbrabant_SWTD\ndatetime64[ns]\n274\n1283\n********\nbrabant_SWTD6\ndatetime64[ns]\n274\n1283\n********\nAmp_CR_New\nobject\n60\n21927\n********\n</pre> In\u00a0[5]: Copied! <pre># Read the data\nds_list = []\nfor file in pickle_files_path:\n    # read files and extract filename\n    df = pd.read_pickle(file)\n    var_name = file.stem\n\n    # Check the dtype of the index. If it's not datetime64[ns], convert it.\n    if df.index.dtype != \"datetime64[ns]\":\n        df.index = pd.to_datetime(df.index)\n\n    # convert dataframe to dataset\n    ds = xr.Dataset({var_name: ([\"time\", \"space\"], df.values)},\n                     coords={\"time\": df.index, \"space\": df.columns})\n    ds_list.append(ds)\n\n# Create one dataset\ndataset = xr.concat(ds_list, dim=\"time\")\n\n# Add attribute (metadata)\ndataset.attrs['source'] = 'data source'\ndataset.attrs['license'] = 'data license'\n</pre> # Read the data ds_list = [] for file in pickle_files_path:     # read files and extract filename     df = pd.read_pickle(file)     var_name = file.stem      # Check the dtype of the index. If it's not datetime64[ns], convert it.     if df.index.dtype != \"datetime64[ns]\":         df.index = pd.to_datetime(df.index)      # convert dataframe to dataset     ds = xr.Dataset({var_name: ([\"time\", \"space\"], df.values)},                      coords={\"time\": df.index, \"space\": df.columns})     ds_list.append(ds)  # Create one dataset dataset = xr.concat(ds_list, dim=\"time\")  # Add attribute (metadata) dataset.attrs['source'] = 'data source' dataset.attrs['license'] = 'data license' In\u00a0[6]: Copied! <pre>dataset\n</pre> dataset Out[6]: <pre>&lt;xarray.Dataset&gt;\nDimensions:        (space: 21927, time: 904)\nCoordinates:\n  * space          (space) int64 1526157 1526199 1526200 ... 2311808 2311820\n  * time           (time) datetime64[ns] 2017-05-06 2017-05-07 ... 2017-12-27\nData variables:\n    brabant_LAI    (time, space) float64 0.0 0.0 0.0 0.0 0.0 ... nan nan nan nan\n    brabant_CWAD   (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    brabant_SWTD   (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    brabant_SWTD6  (time, space) float64 nan nan nan nan nan ... nan nan nan nan\n    Amp_CR_New     (time, space) float64 nan nan nan nan ... 0.445 0.3501 0.3075\nAttributes:\n    source:   data source\n    license:  data license</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>space: 21927</li><li>time: 904</li></ul></li><li>Coordinates: (2)<ul><li>space(space)int641526157 1526199 ... 2311808 2311820<pre>array([1526157, 1526199, 1526200, ..., 2311765, 2311808, 2311820])</pre></li><li>time(time)datetime64[ns]2017-05-06 ... 2017-12-27<pre>array(['2017-05-06T00:00:00.000000000', '2017-05-07T00:00:00.000000000',\n       '2017-05-08T00:00:00.000000000', ..., '2017-12-15T00:00:00.000000000',\n       '2017-12-21T00:00:00.000000000', '2017-12-27T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (5)<ul><li>brabant_LAI(time, space)float640.0 0.0 0.0 0.0 ... nan nan nan nan<pre>array([[ 0.,  0.,  0., ..., nan, nan, nan],\n       [ 0.,  0.,  0., ..., nan, nan, nan],\n       [ 0.,  0.,  0., ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_CWAD(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_SWTD(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>brabant_SWTD6(time, space)float64nan nan nan nan ... nan nan nan nan<pre>array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])</pre></li><li>Amp_CR_New(time, space)float64nan nan nan ... 0.445 0.3501 0.3075<pre>array([[       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       ...,\n       [0.14172222, 0.2719292 , 0.24513262, ..., 0.3101958 , 0.28541936,\n        0.22427617],\n       [0.20507831, 0.32489699, 0.31356163, ..., 0.35787517, 0.32660959,\n        0.29032968],\n       [0.19860778, 0.27824248, 0.27584513, ..., 0.4449813 , 0.35008485,\n        0.30750795]])</pre></li></ul></li><li>Indexes: (2)<ul><li>spacePandasIndex<pre>PandasIndex(Int64Index([1526157, 1526199, 1526200, 1526234, 1526249, 1526374, 1526375,\n            1526429, 1526436, 1526442,\n            ...\n            2311568, 2311595, 2311608, 2311669, 2311688, 2311706, 2311743,\n            2311765, 2311808, 2311820],\n           dtype='int64', name='space', length=21927))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2017-05-06', '2017-05-07', '2017-05-08', '2017-05-09',\n               '2017-05-10', '2017-05-11', '2017-05-12', '2017-05-13',\n               '2017-05-14', '2017-05-15',\n               ...\n               '2017-11-03', '2017-11-09', '2017-11-15', '2017-11-21',\n               '2017-11-27', '2017-12-03', '2017-12-09', '2017-12-15',\n               '2017-12-21', '2017-12-27'],\n              dtype='datetime64[ns]', name='time', length=904, freq=None))</pre></li></ul></li><li>Attributes: (2)source :data sourcelicense :data license</li></ul> In\u00a0[7]: Copied! <pre># Time series of one variable at one location\nAmp_CR_New = dataset.Amp_CR_New.isel(space=0)\nAmp_CR_New.plot()\n</pre> # Time series of one variable at one location Amp_CR_New = dataset.Amp_CR_New.isel(space=0) Amp_CR_New.plot() Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f28a87059d0&gt;]</pre> In\u00a0[8]: Copied! <pre># Save data in netcdf format\ndataset.to_netcdf(nc_file_path)\n</pre> # Save data in netcdf format dataset.to_netcdf(nc_file_path) In\u00a0[9]: Copied! <pre># For large dataset, chunk the data and save data in zarr format\ndataset.chunk({'space':1000})\ndataset.to_zarr(zarr_file_path)\n</pre> # For large dataset, chunk the data and save data in zarr format dataset.chunk({'space':1000}) dataset.to_zarr(zarr_file_path) Out[9]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7f28909af8b0&gt;</pre>"},{"location":"notebooks/example_read_from_multiple_df/#import-libraries-and-set-paths","title":"Import libraries and set paths\u00b6","text":""},{"location":"notebooks/example_read_from_multiple_df/#read-the-data-and-explore-it","title":"Read the data and explore it\u00b6","text":""},{"location":"notebooks/example_read_from_multiple_df/#convert-data","title":"Convert data\u00b6","text":"<p>As seen above, the type of index of data are not consitent among different files e.g. \"object\" and \"datetime64[ns]\". Below, we convert it to the same type.</p>"},{"location":"notebooks/example_read_from_multiple_df/#inspect-output-and-store-it","title":"Inspect output and store it\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/","title":"Covert a nested DataFrame to a Dataset","text":"<p>An example notebook that reads data and convert it to xarray Dataset with aligned time and space axis.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt # only for plots\n</pre> import pandas as pd import xarray as xr import matplotlib.pyplot as plt # only for plots In\u00a0[2]: Copied! <pre>pickle_file_path = \"./example_data/example_data.pickle\"\nnc_file_path = \"./example1_data.nc\"\nzarr_file_path = \"./example1_data.zarr\"\n</pre> pickle_file_path = \"./example_data/example_data.pickle\" nc_file_path = \"./example1_data.nc\" zarr_file_path = \"./example1_data.zarr\" In\u00a0[3]: Copied! <pre># Read the data\ndf_all_gpi = pd.read_pickle(pickle_file_path)\n</pre> # Read the data df_all_gpi = pd.read_pickle(pickle_file_path) In\u00a0[4]: Copied! <pre>df_all_gpi\n</pre> df_all_gpi Out[4]: lat lon data 1 56.125 11.375 sig      slop      curv    ... 2 46.125 6.625 sig      slop      curv    ... 3 53.375 6.125 sig      slop      curv    ... 4 49.375 12.375 sig      slop      curv    ... 5 44.375 0.625 sig      slop      curv    ... In\u00a0[5]: Copied! <pre>df_all_gpi.iloc[3][\"data\"]\n</pre> df_all_gpi.iloc[3][\"data\"] Out[5]: sig slop curv TG1 TG2 TG3 WG1 WG2 WG3 BIOMA1 BIOMA2 datetime_doy 2007-01-02 -8.774847 -0.118061 -0.001871 282.495667 277.571790 280.432019 0.353169 0.297954 0.316928 0.055779 0.064610 2007-01-03 -8.737255 -0.116761 -0.001753 283.059404 278.609833 279.851678 0.224477 0.336283 0.303121 0.057188 0.007182 2007-01-03 -8.791911 -0.118357 -0.002037 284.386143 278.075722 285.383157 0.378645 0.250349 0.335715 0.062280 0.043909 2007-01-05 -7.962205 -0.118063 -0.002072 276.947048 277.841682 277.941320 0.305945 0.332280 0.315607 0.052877 0.017596 2007-01-06 -8.607216 -0.118727 -0.002048 276.458553 282.783491 277.956962 0.380480 0.364697 0.280530 0.051309 0.034444 ... ... ... ... ... ... ... ... ... ... ... ... 2019-12-30 -8.824627 -0.119621 -0.000872 NaN NaN NaN NaN NaN NaN NaN NaN 2019-12-31 -8.578708 -0.121446 -0.001059 NaN NaN NaN NaN NaN NaN NaN NaN 2019-12-31 -8.731547 -0.119538 -0.000887 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 -7.358630 -0.122284 -0.000725 NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 -9.165778 -0.123732 -0.000753 NaN NaN NaN NaN NaN NaN NaN NaN <p>7995 rows \u00d7 11 columns</p> In\u00a0[6]: Copied! <pre># Function to make timestamps unique by adding half an hour\ndef make_timestamps_unique(df):\n    seen_timestamps = set()\n    new_index = []\n\n    for timestamp in df.index:\n        if timestamp not in seen_timestamps:\n            new_index.append(timestamp)\n            seen_timestamps.add(timestamp)\n        else:\n            # Timestamp is a duplicate, add half an hour\n            while timestamp in seen_timestamps:\n                timestamp += pd.Timedelta(minutes=30)\n            new_index.append(timestamp)\n            seen_timestamps.add(timestamp)\n    \n    df.index = new_index\n    df.index.name = \"time\"\n    return df\n</pre> # Function to make timestamps unique by adding half an hour def make_timestamps_unique(df):     seen_timestamps = set()     new_index = []      for timestamp in df.index:         if timestamp not in seen_timestamps:             new_index.append(timestamp)             seen_timestamps.add(timestamp)         else:             # Timestamp is a duplicate, add half an hour             while timestamp in seen_timestamps:                 timestamp += pd.Timedelta(minutes=30)             new_index.append(timestamp)             seen_timestamps.add(timestamp)          df.index = new_index     df.index.name = \"time\"     return df In\u00a0[7]: Copied! <pre>ds_list = []\nfor index, row in df_all_gpi.iterrows():\n    \n    # Filter the nested DataFrame based on location\n    df = df_all_gpi.iloc[index-1][\"data\"]\n\n    # Make timestamps unique\n    df = make_timestamps_unique(df)\n\n    # convert dataframe to dataset\n    ds = xr.Dataset(df, coords={'latitude': row[\"lat\"], 'longitude': row[\"lon\"]})\n    ds_list.append(ds)\n\n# Create one dataset\ndataset = xr.concat(ds_list, dim=\"space\")\n\n# Add attribute (metadata)\ndataset.attrs['source'] = 'data source'\ndataset.attrs['license'] = 'data license'\n</pre> ds_list = [] for index, row in df_all_gpi.iterrows():          # Filter the nested DataFrame based on location     df = df_all_gpi.iloc[index-1][\"data\"]      # Make timestamps unique     df = make_timestamps_unique(df)      # convert dataframe to dataset     ds = xr.Dataset(df, coords={'latitude': row[\"lat\"], 'longitude': row[\"lon\"]})     ds_list.append(ds)  # Create one dataset dataset = xr.concat(ds_list, dim=\"space\")  # Add attribute (metadata) dataset.attrs['source'] = 'data source' dataset.attrs['license'] = 'data license' In\u00a0[8]: Copied! <pre>dataset\n</pre> dataset Out[8]: <pre>&lt;xarray.Dataset&gt;\nDimensions:    (time: 8506, space: 5)\nCoordinates:\n  * time       (time) datetime64[ns] 2007-01-02 ... 2020-01-01T01:00:00\n    latitude   (space) float64 56.12 46.12 53.38 49.38 44.38\n    longitude  (space) float64 11.38 6.625 6.125 12.38 0.625\nDimensions without coordinates: space\nData variables:\n    sig        (space, time) float64 -9.49 -8.494 -9.069 ... -8.071 -8.237\n    slop       (space, time) float64 -0.1208 -0.1178 -0.121 ... -0.1144 -0.1191\n    curv       (space, time) float64 -0.001396 -0.001464 ... -0.0006173\n    TG1        (space, time) float64 280.0 270.4 285.5 277.4 ... nan nan nan nan\n    TG2        (space, time) float64 274.8 278.4 280.6 283.7 ... nan nan nan nan\n    TG3        (space, time) float64 280.9 279.7 278.0 278.0 ... nan nan nan nan\n    WG1        (space, time) float64 0.3249 0.2798 0.2773 0.2867 ... nan nan nan\n    WG2        (space, time) float64 0.3408 0.2902 0.3373 0.2709 ... nan nan nan\n    WG3        (space, time) float64 0.3123 0.2916 0.2891 0.3538 ... nan nan nan\n    BIOMA1     (space, time) float64 0.07079 0.05532 0.04846 ... nan nan nan\n    BIOMA2     (space, time) float64 0.04366 0.0462 0.03821 ... nan nan nan\nAttributes:\n    source:   data source\n    license:  data license</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 8506</li><li>space: 5</li></ul></li><li>Coordinates: (3)<ul><li>time(time)datetime64[ns]2007-01-02 ... 2020-01-01T01:00:00<pre>array(['2007-01-02T00:00:00.000000000', '2007-01-03T00:00:00.000000000',\n       '2007-01-03T00:30:00.000000000', ..., '2020-01-01T00:00:00.000000000',\n       '2020-01-01T00:30:00.000000000', '2020-01-01T01:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>latitude(space)float6456.12 46.12 53.38 49.38 44.38<pre>array([56.125, 46.125, 53.375, 49.375, 44.375])</pre></li><li>longitude(space)float6411.38 6.625 6.125 12.38 0.625<pre>array([11.375,  6.625,  6.125, 12.375,  0.625])</pre></li></ul></li><li>Data variables: (11)<ul><li>sig(space, time)float64-9.49 -8.494 ... -8.071 -8.237<pre>array([[-9.48985745, -8.49424829, -9.06917897, ..., -8.87350746,\n        -8.55944322,         nan],\n       [-9.02591086, -8.79194073, -8.24172421, ..., -8.65639331,\n        -7.39492538, -8.74562845],\n       [-9.29394782, -8.87621932, -8.96752995, ..., -8.2149011 ,\n        -9.08360639,         nan],\n       [-8.77484736, -8.73725534, -8.79191062, ..., -7.35863039,\n        -9.16577772,         nan],\n       [-8.24819331, -8.8397918 , -9.10849921, ..., -8.06904894,\n        -8.07118479, -8.23718386]])</pre></li><li>slop(space, time)float64-0.1208 -0.1178 ... -0.1144 -0.1191<pre>array([[-0.12082425, -0.11783219, -0.12098574, ..., -0.11359581,\n        -0.11493786,         nan],\n       [-0.11527791, -0.11782626, -0.11479579, ..., -0.12395586,\n        -0.12741118, -0.12636578],\n       [-0.11198183, -0.11500686, -0.11297782, ..., -0.11395541,\n        -0.1149019 ,         nan],\n       [-0.11806147, -0.11676077, -0.11835654, ..., -0.1222841 ,\n        -0.1237319 ,         nan],\n       [-0.11225743, -0.1164341 , -0.11506161, ..., -0.11670426,\n        -0.114442  , -0.11907956]])</pre></li><li>curv(space, time)float64-0.001396 -0.001464 ... -0.0006173<pre>array([[-0.00139633, -0.00146407, -0.00155133, ..., -0.00147185,\n        -0.00133919,         nan],\n       [-0.00192987, -0.00172127, -0.00155783, ..., -0.00127703,\n        -0.0010599 , -0.00104103],\n       [-0.00175796, -0.00185267, -0.0018512 , ..., -0.00093651,\n        -0.00089887,         nan],\n       [-0.0018711 , -0.00175281, -0.00203733, ..., -0.00072492,\n        -0.00075293,         nan],\n       [-0.00105648, -0.00112672, -0.0011477 , ..., -0.00039547,\n        -0.00081939, -0.00061725]])</pre></li><li>TG1(space, time)float64280.0 270.4 285.5 ... nan nan nan<pre>array([[280.02112326, 270.43043758, 285.49255157, ...,          nan,\n                 nan,          nan],\n       [273.95243199, 279.10472356, 275.67491766, ...,          nan,\n                 nan,          nan],\n       [282.79039682, 285.25875506, 286.80482212, ...,          nan,\n                 nan,          nan],\n       [282.49566693, 283.05940391, 284.38614309, ...,          nan,\n                 nan,          nan],\n       [272.8261471 , 280.37911393, 283.709762  , ...,          nan,\n                 nan,          nan]])</pre></li><li>TG2(space, time)float64274.8 278.4 280.6 ... nan nan nan<pre>array([[274.84709093, 278.41673812, 280.57289091, ...,          nan,\n                 nan,          nan],\n       [278.42437536, 283.31915753, 278.51042955, ...,          nan,\n                 nan,          nan],\n       [279.21544423, 279.57413371, 284.76804529, ...,          nan,\n                 nan,          nan],\n       [277.57179007, 278.6098334 , 278.07572192, ...,          nan,\n                 nan,          nan],\n       [278.54961327, 283.62250125, 286.60718795, ...,          nan,\n                 nan,          nan]])</pre></li><li>TG3(space, time)float64280.9 279.7 278.0 ... nan nan nan<pre>array([[280.94967399, 279.73701754, 277.95140294, ...,          nan,\n                 nan,          nan],\n       [275.7377006 , 278.96455422, 283.03484774, ...,          nan,\n                 nan,          nan],\n       [277.36359601, 279.51634776, 281.45332132, ...,          nan,\n                 nan,          nan],\n       [280.43201923, 279.85167842, 285.38315711, ...,          nan,\n                 nan,          nan],\n       [280.93250698, 277.34480673, 276.23607919, ...,          nan,\n                 nan,          nan]])</pre></li><li>WG1(space, time)float640.3249 0.2798 0.2773 ... nan nan<pre>array([[0.32492937, 0.27983456, 0.27729615, ...,        nan,        nan,\n               nan],\n       [0.28419109, 0.29665461, 0.26357131, ...,        nan,        nan,\n               nan],\n       [0.28657149, 0.27418057, 0.32130955, ...,        nan,        nan,\n               nan],\n       [0.35316888, 0.22447731, 0.37864479, ...,        nan,        nan,\n               nan],\n       [0.32013111, 0.32989414, 0.37377789, ...,        nan,        nan,\n               nan]])</pre></li><li>WG2(space, time)float640.3408 0.2902 0.3373 ... nan nan<pre>array([[0.34075338, 0.29023887, 0.33732519, ...,        nan,        nan,\n               nan],\n       [0.29739347, 0.31481947, 0.32647732, ...,        nan,        nan,\n               nan],\n       [0.31802497, 0.35188802, 0.27310549, ...,        nan,        nan,\n               nan],\n       [0.29795442, 0.33628332, 0.25034937, ...,        nan,        nan,\n               nan],\n       [0.34499846, 0.3227702 , 0.34632033, ...,        nan,        nan,\n               nan]])</pre></li><li>WG3(space, time)float640.3123 0.2916 0.2891 ... nan nan<pre>array([[0.31225323, 0.29158948, 0.28909846, ...,        nan,        nan,\n               nan],\n       [0.31240911, 0.28942008, 0.28934139, ...,        nan,        nan,\n               nan],\n       [0.31223442, 0.32122713, 0.25863411, ...,        nan,        nan,\n               nan],\n       [0.31692828, 0.30312131, 0.33571518, ...,        nan,        nan,\n               nan],\n       [0.34937695, 0.33768108, 0.3156577 , ...,        nan,        nan,\n               nan]])</pre></li><li>BIOMA1(space, time)float640.07079 0.05532 0.04846 ... nan nan<pre>array([[0.07078687, 0.05531632, 0.04845626, ...,        nan,        nan,\n               nan],\n       [0.06313461, 0.06849046, 0.06948795, ...,        nan,        nan,\n               nan],\n       [0.10973646, 0.09282028, 0.10490551, ...,        nan,        nan,\n               nan],\n       [0.05577876, 0.05718846, 0.06228029, ...,        nan,        nan,\n               nan],\n       [0.06641911, 0.07987195, 0.07466871, ...,        nan,        nan,\n               nan]])</pre></li><li>BIOMA2(space, time)float640.04366 0.0462 0.03821 ... nan nan<pre>array([[0.04365577, 0.04620425, 0.03821127, ...,        nan,        nan,\n               nan],\n       [0.05440495, 0.03978223, 0.00253587, ...,        nan,        nan,\n               nan],\n       [0.11810849, 0.13832198, 0.1520621 , ...,        nan,        nan,\n               nan],\n       [0.06460992, 0.00718201, 0.04390921, ...,        nan,        nan,\n               nan],\n       [0.04191187, 0.04335878, 0.08054077, ...,        nan,        nan,\n               nan]])</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2007-01-02 00:00:00', '2007-01-03 00:00:00',\n               '2007-01-03 00:30:00', '2007-01-05 00:00:00',\n               '2007-01-06 00:00:00', '2007-01-08 00:00:00',\n               '2007-01-08 00:30:00', '2007-01-09 00:00:00',\n               '2007-01-10 00:00:00', '2007-01-11 00:00:00',\n               ...\n               '2019-12-29 01:00:00', '2019-12-30 00:00:00',\n               '2019-12-30 00:30:00', '2019-12-30 01:00:00',\n               '2019-12-30 01:30:00', '2019-12-31 00:00:00',\n               '2019-12-31 00:30:00', '2020-01-01 00:00:00',\n               '2020-01-01 00:30:00', '2020-01-01 01:00:00'],\n              dtype='datetime64[ns]', name='time', length=8506, freq=None))</pre></li></ul></li><li>Attributes: (2)source :data sourcelicense :data license</li></ul> In\u00a0[9]: Copied! <pre># Time series of one variable at one location\nsig = dataset.sig.isel(space=0)\nsig.plot()\n</pre> # Time series of one variable at one location sig = dataset.sig.isel(space=0) sig.plot() Out[9]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fe3818fb510&gt;]</pre> In\u00a0[10]: Copied! <pre># map of one variable at one time\ntime = \"2007-01-02T00:00:00.000000000\"\nsig = dataset.sig.sel(time=time)\nlons, lats = xr.broadcast(sig.longitude, sig.latitude)\nplt.figure(figsize=(8, 6))\nplt.scatter(lons, lats, c=sig, cmap=\"viridis\", marker=\"o\", s=10)\nplt.colorbar(label=\"sig\") \nplt.xlabel(\"longitude\")\nplt.ylabel(\"latitude\")\nplt.title(f\"Plot of sig at time {time}\")\nplt.grid(True)\nplt.show()\n</pre> # map of one variable at one time time = \"2007-01-02T00:00:00.000000000\" sig = dataset.sig.sel(time=time) lons, lats = xr.broadcast(sig.longitude, sig.latitude) plt.figure(figsize=(8, 6)) plt.scatter(lons, lats, c=sig, cmap=\"viridis\", marker=\"o\", s=10) plt.colorbar(label=\"sig\")  plt.xlabel(\"longitude\") plt.ylabel(\"latitude\") plt.title(f\"Plot of sig at time {time}\") plt.grid(True) plt.show() In\u00a0[11]: Copied! <pre># Save data in netcdf format\ndataset.to_netcdf(nc_file_path)\n</pre> # Save data in netcdf format dataset.to_netcdf(nc_file_path) In\u00a0[12]: Copied! <pre># For large dataset, chunk the data and save data in zarr format\ndataset.chunk({'space':1000})\ndataset.to_zarr(zarr_file_path)\n</pre> # For large dataset, chunk the data and save data in zarr format dataset.chunk({'space':1000}) dataset.to_zarr(zarr_file_path) Out[12]: <pre>&lt;xarray.backends.zarr.ZarrStore at 0x7fe2fffda420&gt;</pre>"},{"location":"notebooks/example_read_from_one_df/#import-libraries-and-set-paths","title":"Import libraries and set paths\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/#read-the-data-and-explore-it","title":"Read the data and explore it\u00b6","text":""},{"location":"notebooks/example_read_from_one_df/#convert-data","title":"Convert data\u00b6","text":"<p>As seen above, the \"datetime_doy\" values are not unique. While it's possible to have non-unique index values, it's generally not recommended. Having a non-unique index can make certain operations and data manipulation more complex, or even incorrect. These values shows two observation at one day. To avoid duplication, we add a hal-hour shift.</p>"},{"location":"notebooks/example_read_from_one_df/#inspect-output-and-store-it","title":"Inspect output and store it\u00b6","text":""}]}